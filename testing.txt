from __future__ import annotations
import os, json, re, time, base64, io
from collections import defaultdict
from datetime import datetime, date, timedelta
from typing import Dict, Any, List, Tuple, Optional, Literal, Union

from dotenv import load_dotenv
import pandas as pd
import numpy as np
from matplotlib.figure import Figure

from pydantic import BaseModel, Field, ValidationError

try:
    from json_repair import repair_json as _repair_json_lib
except Exception:
    _repair_json_lib = None
    
try:
    from dirtyjson import loads as _dirtyjson_loads
except Exception:
    _dirtyjson_loads = None

try:
    from openai import OpenAI as _OpenAI
except Exception:
    _OpenAI = None

load_dotenv()

# -------------------- COMMON / EXISTING ENV --------------------
SEED           = int(os.getenv("SEED", "42"))
TEMPERATURE    = float(os.getenv("TEMPERATURE", "0.2"))
TOP_P          = float(os.getenv("TOP_P", "0.9"))
XAI_OUTPUT_XLS = os.getenv("XAI_OUTPUT_PATH", "./xai_outputs.xlsx")
XAI_DATA_PATH  = os.getenv("XAI_DATA_PATH", "./Book1.xlsx")
XAI_MAX_DRIVERS= int(os.getenv("XAI_MAX_DRIVERS", "4"))  # rule-based fallback top-N
XAI_CONTEXT_JSON = os.getenv("XAI_CONTEXT_JSON", "").strip()

XAI_OVERALL_MODE = os.getenv("XAI_OVERALL_MODE", "hybrid").strip().lower()  # 'hybrid' | 'llm' | 'rb'

DATABRICKS_BASE_URL = (os.getenv("DATABRICKS_BASE_URL", "") or "").strip().rstrip("/")
DATABRICKS_TOKEN = os.getenv("DATABRICKS_TOKEN", "").strip()
DATABRICKS_MODEL = os.getenv("DATABRICKS_MODEL", "").strip()

XAI_INCLUDE_MODEL_OUTPUTS = os.getenv("XAI_INCLUDE_MODEL_OUTPUTS", "1").strip() == "1"

XAI_CONTEXT_SCHEMA_PATH = os.getenv("XAI_CONTEXT_SCHEMA_PATH", "./xai_context_schema.json")
try:
    with open(XAI_CONTEXT_SCHEMA_PATH, "r", encoding="utf-8") as _f:
        XAI_CONTEXT_SCHEMA = json.load(_f)
except Exception:
    XAI_CONTEXT_SCHEMA = None

REASONING_MODEL_NAMES = {
    "o1", "o1-mini", "o1-preview",
    "o3", "o3-mini",
    "o4", "o4-mini",
}

STANDARD_MODEL_NAMES = {
    "gpt-4.1-mini", "gpt-4.1",
    "gpt-4o", "gpt-4o-mini",
    "gpt-3.5-turbo",
}

def _is_reasoning_model_name(name: str) -> bool:
    """
    Decide if a model should be treated as a reasoning model using explicit lists
    of reasoning and standard model names.
    """
    if name in REASONING_MODEL_NAMES:
        return True
    if name in STANDARD_MODEL_NAMES:
        return False
    # Unknown models default to standard behaviour
    return False

# Global flag for the active backend/model
IS_REASONING_MODEL = _is_reasoning_model_name(DATABRICKS_MODEL)

# Metrics / token estimation
XAI_SAVE_METRICS = os.getenv("XAI_SAVE_METRICS", "1").strip() == "1"
XAI_METRICS_CSV = os.getenv("XAI_METRICS_CSV", "./xai_llm_metrics.csv")

# NEW: limits for tabular + plot context
XAI_MAX_TRAIN_ROWS = int(os.getenv("XAI_MAX_TRAIN_ROWS", "10"))
XAI_MAX_TRAIN_FEATURES = int(os.getenv("XAI_MAX_TRAIN_FEATURES", "8"))
XAI_MAX_SCATTER_FEATURES = int(os.getenv("XAI_MAX_SCATTER_FEATURES", "4"))

try:
    import tiktoken as _tiktoken
    _tk_enc = _tiktoken.get_encoding("cl100k_base")
except Exception:
    _tk_enc = None

def _estimate_tokens(text: str) -> int:
    if not text:
        return 0
    try:
        if _tk_enc is not None:
            return len(_tk_enc.encode(text))
    except Exception:
        pass
    return max(1, int(len(text) / 4))

# Global metrics log
LLM_METRICS_LOG: List[Dict[str, Any]] = []
LAST_CALL_METRICS: Dict[str, Any] = {}


def aggregate_weekly_flat_to_monthly_mean(
    weekly_flat: Dict[str, Any],
    out_months: int = 4
):
    """
    Vectorized monthly aggregation using pandas:

    weekly_flat: {
      "0": {"Group": "A", "Date": "YYYY-MM-DD", "grp_contri"/"Grp_contri": ...},
      "1": {...}, ...
    }

    Returns:
      monthly_flat: {
        "0": {"Group": ..., "Date": "YYYY-MM-01", "Avg_grp_contri": ...},
        ...
      },
      groups_sorted: [group1, group2, ...],
      months_horizon: ["YYYY-MM", ...]  # length = out_months
    """
    if not isinstance(weekly_flat, dict) or not weekly_flat:
        raise ValueError("Weekly JSON is empty or not a dict")

    df = pd.DataFrame.from_dict(weekly_flat, orient="index")

    if "Group" not in df.columns or "Date" not in df.columns:
        raise ValueError("Weekly JSON must have 'Group' and 'Date' fields")

    # Find contribution column
    contrib_col = None
    for cand in ("grp_contri", "Grp_contri", "grp_contribution", "Grp_contribution"):
        if cand in df.columns:
            contrib_col = cand
            break
    if contrib_col is None:
        raise ValueError("Weekly JSON missing contribution column (e.g. 'grp_contri')")

    df = df[["Group", "Date", contrib_col]].copy()
    df["Date"] = pd.to_datetime(df["Date"])
    df["Grp_contri"] = pd.to_numeric(df[contrib_col], errors="coerce")
    df = df.dropna(subset=["Grp_contri"])

    if df.empty:
        raise ValueError("Weekly JSON has no valid numeric contributions")

    # Monthly period
    df["Period"] = df["Date"].dt.to_period("M")
    start_period = df["Period"].min()
    if pd.isna(start_period):
        raise ValueError("Unable to determine start month from 'Date'")

    # Build horizon of consecutive months
    periods = [start_period + i for i in range(out_months)]
    groups_sorted = sorted(df["Group"].astype(str).unique().tolist())

    # Full grid: all groups x all months, fill missing with 0
    idx = pd.MultiIndex.from_product(
        [groups_sorted, periods],
        names=["Group", "Period"]
    )

    monthly = (
        df.groupby(["Group", "Period"])["Grp_contri"]
          .mean()
          .reindex(idx, fill_value=0.0)
          .reset_index()
    )

    monthly_flat: Dict[str, Any] = {}
    for i, row in monthly.iterrows():
        period: pd.Period = row["Period"]
        monthly_flat[str(i)] = {
            "Group": row["Group"],
            "Date": period.to_timestamp(how="start").strftime("%Y-%m-%d"),
            "Avg_grp_contri": round(float(row["Grp_contri"]), 6),
        }

    months_horizon = [str(p) for p in periods]
    return monthly_flat, groups_sorted, months_horizon

def build_label_policy_from_monthly(monthly_flat: Dict[str, Any]):
    """
    Use numpy.percentile instead of manual percentile logic.
    """
    mags = np.array(
        [abs(float(rec["Avg_grp_contri"])) for rec in monthly_flat.values()],
        dtype=float,
    )

    if mags.size == 0:
        p50, p80 = 0.0, 0.0
    else:
        p50, p80 = np.percentile(mags, [50, 80])

    return {
        "slightly":      {"lt": float(p50)},
        "moderately":    {"ge": float(p50), "lt": float(p80)},
        "significantly": {"ge": float(p80)},
    }


# ------------- FACTS PREPARATION FOR LLM ------------------
def month_label(ym: str) -> str:
    return datetime.strptime(ym+"-01", "%Y-%m-%d").strftime("%b %Y")

def build_group_trends(payload_months: List[Dict[str, Any]]) -> Dict[str, Any]:
    by_group = defaultdict(list)
    for m in payload_months:
        for g in m["groups"]:
            by_group[g["name"]].append(g["value"])
    out = {}
    for g, vals in by_group.items():
        sign_changes = sum(1 for i in range(1, len(vals)) if (vals[i] > 0) != (vals[i-1] > 0))
        out[g] = {
            "mean": round(sum(vals)/len(vals), 3),
            "share_positive": round(sum(1 for v in vals if v > 0)/len(vals), 2),
            "sign_changes": sign_changes
        }
    return out

def build_month_balance(payload_months: List[Dict[str, Any]]) -> Dict[str, Any]:
    out = {}
    for m in payload_months:
        ym = m["month"]
        vals = [abs(g["value"]) for g in m["groups"]]
        sum_abs = sum(vals)
        net = abs(m["net_effect"])
        ratio = net / sum_abs if sum_abs else 0.0
        if ratio < 0.30: label = "mostly offsetting"
        elif ratio < 0.60: label = "mixed"
        else: label = "concentrated"
        out[ym] = {"sum_abs": round(sum_abs, 3), "balance_ratio": round(ratio, 3), "balance_label": label}
    return out

def build_prev_map(months_list: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
    prev_map = {}
    for i, m in enumerate(months_list):
        if i == 0:
            prev_map[m["month"]] = {}
        else:
            prev = months_list[i-1]
            prev_map[m["month"]] = {g["name"]: float(g["value"]) for g in prev["groups"]}
    return prev_map

def build_weekly_recs_from_grouped(weekly_flat: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    Collapse SHAP_Weekly_Grouped-style JSON into per-date records:

    weekly_flat: {
      "0": {"Group": "A", "Date": "YYYY-MM-DD", "Grp_contri"/"grp_contri": ...},
      "1": {...},
      ...
    }

    Returns:
      {
        "YYYY-MM-DD": {
          "date": "YYYY-MM-DD",
          "groups": [ {"name": <Group>, "value": float}, ... ],
          "net_effect": float
        },
        ...
      }
    """
    if not isinstance(weekly_flat, dict):
        return {}

    by_date: Dict[str, List[Dict[str, float]]] = defaultdict(list)
    for v in weekly_flat.values():
        if not isinstance(v, dict):
            continue
        dval = v.get("Date")
        g = v.get("Group")
        if not dval or not g:
            continue
        dstr = str(dval)[:10]
        contrib = v.get("Grp_contri", v.get("grp_contri"))
        if contrib is None:
            continue
        try:
            val = float(contrib)
        except Exception:
            continue
        by_date[dstr].append({"name": g, "value": val})

    weekly_recs: Dict[str, Dict[str, Any]] = {}
    for d, groups in by_date.items():
        net = sum(g["value"] for g in groups)
        weekly_recs[d] = {
            "date": d,
            "groups": groups,
            "net_effect": round(net, 4),
        }
    return weekly_recs

def build_weekly_prev_map(weekly_recs: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
    """
    For each date, map to previous date's group values so _rb_summarize can
    describe deltas vs prior week.
    """
    dates = sorted(weekly_recs.keys())
    prev_map: Dict[str, Dict[str, float]] = {}
    for i, d in enumerate(dates):
        if i == 0:
            prev_map[d] = {}
        else:
            prev_rec = weekly_recs[dates[i - 1]]
            prev_vals = {g["name"]: float(g["value"]) for g in prev_rec.get("groups", [])}
            prev_map[d] = prev_vals
    return prev_map

# ----------------- EXTRA CONTEXT: MODEL & TRAIN DATA -----------------
def _safe_parse_json_field(val: Any) -> Any:
    if isinstance(val, (dict, list)):
        return val
    if isinstance(val, str):
        txt = val.strip()
        if not txt:
            return {}
        try:
            return json.loads(txt)
        except Exception:
            if _repair_json_lib is not None:
                try:
                    repaired = _repair_json_lib(txt)
                    return json.loads(repaired)
                except Exception:
                    pass
    return val

def _plot_y_train_data_url(y_df: pd.DataFrame, date_col: str, target_col: str) -> str:
    """
    Plot y_train time series and return a data URL string suitable for OpenAI
    multimodal 'image_url' content. This is NOT embedded in the XAI JSON payload.
    """
    if y_df is None or y_df.empty:
        return ""

    df = y_df.copy()
    try:
        df[date_col] = pd.to_datetime(df[date_col])
        df = df.sort_values(date_col)
        x = df[date_col]
        xlabel = date_col
    except Exception:
        x = list(range(len(df)))
        xlabel = "index"

    fig = Figure(figsize=(6, 3))
    ax = fig.subplots()
    ax.plot(x, df[target_col])
    ax.set_xlabel(xlabel)
    ax.set_ylabel(target_col)
    ax.set_title("y_train time series")
    fig.tight_layout()

    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight")
    b64 = base64.b64encode(buf.getvalue()).decode("ascii")
    return f"data:image/png;base64,{b64}"


def _plot_x_train_scatter_data_url(x_df: pd.DataFrame, date_col: str, feature_cols: List[str]) -> str:
    """
    Plot X_train features vs time and return a data URL string. Only used for
    multimodal image input, not embedded inside JSON.
    """
    if x_df is None or x_df.empty:
        return ""

    df = x_df.copy()
    try:
        df[date_col] = pd.to_datetime(df[date_col])
        df = df.sort_values(date_col)
        x = df[date_col]
        xlabel = date_col
    except Exception:
        x = list(range(len(df)))
        xlabel = "index"

    sel_features = [c for c in feature_cols[:XAI_MAX_SCATTER_FEATURES] if c in df.columns]
    if not sel_features:
        return ""

    fig = Figure(figsize=(6, 3))
    ax = fig.subplots()
    for feat in sel_features:
        ax.scatter(x, df[feat], s=10, label=feat)

    ax.set_xlabel(xlabel)
    ax.set_ylabel("feature value")
    ax.set_title("X_train feature scatter")
    if len(sel_features) > 1:
        ax.legend(loc="best", fontsize="xx-small")
    fig.tight_layout()

    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight")
    b64 = base64.b64encode(buf.getvalue()).decode("ascii")
    return f"data:image/png;base64,{b64}"


def _plot_trained_data_data_url(train_df: pd.DataFrame) -> str:
    """
    Plot Actuals_Train vs Fitted_Train over Date (or index fallback) and return
    a data URL string for multimodal input.
    """
    if train_df is None or train_df.empty:
        return ""

    df = train_df.copy()

    # X-axis: Date if available and parseable, else index
    if "Date" in df.columns:
        try:
            df["Date"] = pd.to_datetime(df["Date"])
            df = df.sort_values("Date")
            x = df["Date"]
            xlabel = "Date"
        except Exception:
            x = list(range(len(df)))
            xlabel = "index"
    else:
        x = list(range(len(df)))
        xlabel = "index"

    for col in ("Actuals_Train", "Fitted_Train"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    fig = Figure(figsize=(6, 3))
    ax = fig.subplots()
    if "Actuals_Train" in df.columns:
        ax.plot(x, df["Actuals_Train"], label="Actuals train")
    if "Fitted_Train" in df.columns:
        ax.plot(x, df["Fitted_Train"], linestyle="--", label="Fitted train")

    ax.set_xlabel(xlabel)
    ax.set_ylabel("value")
    ax.set_title("Train fit: actual vs fitted")
    if "Actuals_Train" in df.columns and "Fitted_Train" in df.columns:
        ax.legend(loc="best", fontsize="xx-small")
    fig.tight_layout()

    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight")
    b64 = base64.b64encode(buf.getvalue()).decode("ascii")
    return f"data:image/png;base64,{b64}"



def _trained_data_to_df(trained: Any) -> Optional[pd.DataFrame]:
    """
    Best-effort parser for Trained_Data into a tidy DataFrame with
    columns ['Date', 'Actuals_Train', 'Fitted_Train'] using pandas'
    polymorphic constructors.

    Supports:
    - dict-of-dicts: {date: {Actuals_Train:..., Fitted_Train:...}, ...}
    - columnar dict: {col_name: [..], ...}
    - list-of-dicts: [{Date:..., Actuals_Train:..., Fitted_Train:...}, ...]
    """
    if trained is None:
        return None

    df: Optional[pd.DataFrame]

    if isinstance(trained, dict):
        if not trained:
            return None
        # Columnar dict
        if all(isinstance(v, list) for v in trained.values()):
            df = pd.DataFrame(trained)
        else:
            # dict-of-dicts keyed by date
            df = pd.DataFrame.from_dict(trained, orient="index")
            df = df.reset_index().rename(columns={"index": "Date"})
    elif isinstance(trained, list):
        if not trained:
            return None
        df = pd.DataFrame(trained)
    else:
        return None

    if df is None or df.empty:
        return None

    # Normalize column names
    rename: Dict[str, str] = {}
    for col in df.columns:
        lc = str(col).strip().lower().replace(" ", "_")
        if ("date" in lc or lc in ("ds", "timestamp")) and "Date" not in rename.values():
            rename[col] = "Date"
        elif ("actual" in lc or "y_train" in lc) and "Actuals_Train" not in rename.values():
            rename[col] = "Actuals_Train"
        elif any(tok in lc for tok in ("fitted", "predict", "prediction", "yhat", "y_fitted")) \
                and "Fitted_Train" not in rename.values():
            rename[col] = "Fitted_Train"

    if rename:
        df = df.rename(columns=rename)

    cols: List[str] = []
    if "Date" in df.columns:
        cols.append("Date")
    if "Actuals_Train" in df.columns:
        cols.append("Actuals_Train")
    if "Fitted_Train" in df.columns:
        cols.append("Fitted_Train")

    if not cols:
        return None

    return df[cols]

def _model_outputs_to_df(outputs: Any) -> Optional[pd.DataFrame]:
    """
    Normalize Model_Outputs into a DataFrame with at least
    ['Date', 'Actuals', 'Predictions'] columns, if possible.
    """
    records: List[Dict[str, Any]] = []

    if isinstance(outputs, dict):
        for k, v in outputs.items():
            if not isinstance(v, dict):
                continue
            actual = v.get("Actuals", v.get("Actual", v.get("y_test")))
            pred = v.get("Predictions", v.get("Prediction", v.get("y_pred")))
            if actual is None and pred is None:
                continue
            records.append({"Date": k, "Actuals": actual, "Predictions": pred})

    elif isinstance(outputs, list):
        for item in outputs:
            if not isinstance(item, dict):
                continue
            date_val = item.get("Date") or item.get("date")
            actual = item.get("Actuals") or item.get("Actual") or item.get("y_test")
            pred = item.get("Predictions") or item.get("Prediction") or item.get("y_pred")
            if actual is None and pred is None:
                continue
            records.append({"Date": date_val, "Actuals": actual, "Predictions": pred})

    else:
        return None

    if not records:
        return None

    df = pd.DataFrame(records)
    return df


def _df_to_markdown_table(
    df: pd.DataFrame,
    max_rows: int = XAI_MAX_TRAIN_ROWS,
    max_cols: int = 8
) -> str:
    """
    Small helper around pandas.to_markdown for XAI context.

    - Truncates to max_rows / max_cols.
    - Uses GitHub-style markdown.
    - Falls back to df.to_string(...) if markdown dependencies are missing.
    """
    if df is None or df.empty:
        return ""
    df2 = df.iloc[:max_rows, :max_cols]
    try:
        return df2.to_markdown(index=False, tablefmt="github")
    except Exception:
        return df2.to_string(index=False)


def _summarize_model_outputs(outputs: Any) -> Tuple[str, Dict[str, Any]]:
    """
    Build a small markdown table + basic error stats from model_outputs,
    where Actuals ≈ y_test and Predictions ≈ y_pred.
    """
    df = _model_outputs_to_df(outputs)
    if df is None or df.empty:
        return "", {}

    for col in ("Actuals", "Predictions"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    if "Actuals" in df.columns and "Predictions" in df.columns:
        df["Error"] = df["Predictions"] - df["Actuals"]
        mae = float(df["Error"].abs().mean())
        rmse = float((df["Error"]**2).mean() ** 0.5)
    else:
        mae = None
        rmse = None

    table = _df_to_markdown_table(df, max_rows=XAI_MAX_TRAIN_ROWS, max_cols=len(df.columns))
    summary: Dict[str, Any] = {"n_points": int(len(df))}
    if mae is not None:
        summary["mae"] = round(mae, 6)
        summary["rmse"] = round(rmse, 6)
    return table, summary

def _normalize_row_date(row: pd.Series) -> Optional[str]:
    """
    Try to extract the row's Date column as 'YYYY-MM-DD' string
    using pandas' datetime parsing.
    """
    date_col = next((c for c in row.index if c.strip().lower() == "date"), None)
    if not date_col:
        return None

    val = row[date_col]

    # Fast path for datetime/date (Timestamp subclasses datetime)
    if isinstance(val, (datetime, date)):
        return val.strftime("%Y-%m-%d")

    try:
        dt = pd.to_datetime(val, errors="coerce")
    except Exception:
        return None

    if pd.isna(dt):
        return None

    return dt.strftime("%Y-%m-%d")


def _json_get_path(obj: Any, path: str) -> Any:
    """
    Very small JSON-path helper for context schema mapping.
    Supports dotted paths like 'LearnedParams.n_features_in'.
    """
    if obj is None or path is None:
        return None
    if not isinstance(obj, dict):
        return None
    cur: Any = obj
    for part in str(path).split("."):
        if not isinstance(cur, dict) or part not in cur:
            return None
        cur = cur[part]
    return cur


def _apply_schema_object(raw_obj: Any, col_schema: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Apply an 'object' type schema for a single column (e.g. Model_Details).
    Expects:
      col_schema = {
        "type": "object",
        "fields": [
          {"name": "...", "json_path": "A.B.C"},
          ...
        ]
      }
    """
    if not isinstance(col_schema, dict):
        return None
    fields = col_schema.get("fields") or []
    if not fields:
        return None

    if not isinstance(raw_obj, dict):
        try:
            raw_obj = _safe_parse_json_field(raw_obj)
        except Exception:
            return None
    if not isinstance(raw_obj, dict):
        return None

    out: Dict[str, Any] = {}
    for f in fields:
        name = f.get("name")
        path = f.get("json_path")
        if not name or not path:
            continue
        val = _json_get_path(raw_obj, path)
        if val is not None:
            out[name] = val
    return out or None


def _apply_schema_timeseries(df: Optional[pd.DataFrame], col_schema: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Apply a 'time_series' schema on a DataFrame (e.g. Trained_Data, Model_Outputs).

    Expects:
      col_schema = {
        "type": "time_series",
        "fields": [
          {"name": "...", "agg": "len"},
          {"name": "...", "agg": "min"|"max"|"mean", "column": "Col"},
          {"name": "...", "agg": "mae"|"rmse"|"mean_error", "columns": ["Actual", "Pred"]},
          ...
        ]
      }
    """
    if df is None or df.empty or not isinstance(col_schema, dict):
        return None
    fields = col_schema.get("fields") or []
    if not fields:
        return None

    out: Dict[str, Any] = {}
    for f in fields:
        name = f.get("name")
        agg = (f.get("agg") or "").lower()
        if not name or not agg:
            continue

        cols = f.get("columns") or []
        col = f.get("column")

        try:
            if agg == "len":
                out[name] = int(len(df))

            elif agg in ("min", "max", "mean") and col:
                if col in df.columns:
                    series = pd.to_numeric(df[col], errors="coerce")
                    if agg == "min":
                        out[name] = float(series.min())
                    elif agg == "max":
                        out[name] = float(series.max())
                    else:
                        out[name] = float(series.mean())

            elif agg in ("mae", "rmse", "mean_error") and len(cols) == 2:
                c_actual, c_pred = cols
                if c_actual in df.columns and c_pred in df.columns:
                    a = pd.to_numeric(df[c_actual], errors="coerce")
                    p = pd.to_numeric(df[c_pred], errors="coerce")
                    err = p - a
                    if agg == "mae":
                        out[name] = float(err.abs().mean())
                    elif agg == "rmse":
                        out[name] = float((err**2).mean() ** 0.5)
                    else:  # mean_error
                        out[name] = float(err.mean())
        except Exception:
            # Swallow per-field errors; keep whatever else we managed to compute
            continue

    return out or None


def _apply_context_schema(
    column_name: str,
    raw_value: Any = None,
    df_value: Optional[pd.DataFrame] = None
) -> Optional[Dict[str, Any]]:
    """
    Apply xai_context_schema.json for a given logical column
    (e.g. 'Model_Details', 'Trained_Data', 'Model_Outputs').

    - For type 'object', raw_value is used with json_path.
    - For type 'time_series', df_value is used for numeric aggregations.
    """
    if not XAI_CONTEXT_SCHEMA or not isinstance(XAI_CONTEXT_SCHEMA, dict):
        return None

    cols_cfg = XAI_CONTEXT_SCHEMA.get("columns") or {}
    col_schema = cols_cfg.get(column_name)
    if not isinstance(col_schema, dict):
        return None

    ctype = (col_schema.get("type") or "object").lower()
    if ctype == "object":
        return _apply_schema_object(raw_value, col_schema)
    if ctype == "time_series":
        return _apply_schema_timeseries(df_value, col_schema)
    return None


def _extract_train_metrics_from_model_details(model_details: Any) -> Dict[str, float]:
    """
    Pull train-side fit metrics (RMSE, MAE, R^2) out of Model_Details.

    Assumes metrics are stored either at the top level or inside
    a 'LearnedParams' / 'Metrics'-like dict. Values are based on
    y_train vs y_fitted (.predict(X_train)).
    """
    out: Dict[str, float] = {}
    if not isinstance(model_details, dict):
        return out

    candidates: List[Dict[str, Any]] = [model_details]
    lp = model_details.get("LearnedParams")
    if isinstance(lp, dict):
        candidates.append(lp)
    metrics = model_details.get("Metrics")
    if isinstance(metrics, dict):
        candidates.append(metrics)

    for d in candidates:
        for key, val in d.items():
            lk = str(key).lower()
            try:
                v = float(val)
            except Exception:
                continue

            if lk in ("rmse", "train_rmse") or lk.endswith("_rmse"):
                out.setdefault("train_rmse", v)
            elif lk in ("mae", "train_mae") or lk.endswith("_mae"):
                out.setdefault("train_mae", v)
            elif lk in ("r2", "r2_score", "r_squared"):
                out.setdefault("train_r2", v)

    return out


def _build_regression_summary(model_details: Any) -> Optional[Dict[str, Any]]:
    """
    Build a compact regression summary, mainly for linear regression:
    - model_type
    - n_features
    - intercept
    - top features by |coef|
    - embedded train metrics (RMSE/MAE/R^2)
    """
    if not isinstance(model_details, dict):
        return None

    model_type = model_details.get("ModelType") or model_details.get("model_type")
    lp = model_details.get("LearnedParams") or {}
    if not isinstance(lp, dict):
        lp = {}

    coefs = lp.get("coef_")
    feat_names = lp.get("features_name_in") or lp.get("feature_names_in")
    intercept = lp.get("intercept_", model_details.get("intercept_"))

    try:
        coef_list = list(coefs) if coefs is not None else []
    except Exception:
        coef_list = []

    try:
        name_list = list(feat_names) if feat_names is not None else []
    except Exception:
        name_list = []

    top_features: List[Dict[str, Any]] = []
    if coef_list and name_list and len(coef_list) == len(name_list):
        tmp = []
        for c, n in zip(coef_list, name_list):
            try:
                cf = float(c)
            except Exception:
                continue
            tmp.append({
                "name": str(n),
                "coef": cf,
                "abs_coef": abs(cf),
                "sign": "positive" if cf >= 0 else "negative",
            })
        tmp_sorted = sorted(tmp, key=lambda x: x["abs_coef"], reverse=True)
        top_features = tmp_sorted[: min(5, len(tmp_sorted))]

    train_metrics = _extract_train_metrics_from_model_details(model_details)

    if not (model_type or top_features or train_metrics):
        return None

    out: Dict[str, Any] = {
        "model_type": model_type,
        "n_features": lp.get("n_features_in") or len(top_features) or None,
        "intercept": float(intercept) if intercept is not None else None,
        "top_features": top_features,
        "train_metrics": train_metrics or None,
    }
    return out


def _build_fit_diagnostics(
    model_details: Any,
    model_outputs_summary: Optional[Dict[str, Any]]
) -> Optional[Dict[str, Any]]:
    """
    Combine train metrics from Model_Details with test metrics
    from model_outputs_summary (which is based on full X_test/y_test),
    and compute simple ratios + regime flags.

    This is what the LLM should use to talk about overfitting / degradation.
    """
    train = _extract_train_metrics_from_model_details(model_details) if model_details else {}
    test: Dict[str, float] = {}

    if isinstance(model_outputs_summary, dict):
        try:
            if "mae" in model_outputs_summary:
                test["test_mae"] = float(model_outputs_summary["mae"])
            if "rmse" in model_outputs_summary:
                test["test_rmse"] = float(model_outputs_summary["rmse"])
        except Exception:
            pass

    if not train and not test:
        return None

    diag: Dict[str, Any] = {
        "train": train or None,
        "test": test or None,
        "ratios": {},
    }

    ratios: Dict[str, float] = {}
    try:
        tr_rmse = train.get("train_rmse")
        te_rmse = test.get("test_rmse")
        if tr_rmse and te_rmse and tr_rmse > 0:
            ratios["rmse_test_to_train"] = float(te_rmse) / float(tr_rmse)
    except Exception:
        pass

    try:
        tr_mae = train.get("train_mae")
        te_mae = test.get("test_mae")
        if tr_mae and te_mae and tr_mae > 0:
            ratios["mae_test_to_train"] = float(te_mae) / float(tr_mae)
    except Exception:
        pass

    if ratios:
        diag["ratios"] = ratios

        flags: Dict[str, str] = {}
        rr = ratios.get("rmse_test_to_train")
        if rr is not None:
            if rr >= 1.5:
                flags["rmse_regime"] = "higher_error_than_training"
            elif rr <= 0.7:
                flags["rmse_regime"] = "easier_than_training"
            else:
                flags["rmse_regime"] = "similar_to_training"

        mr = ratios.get("mae_test_to_train")
        if mr is not None:
            if mr >= 1.5:
                flags["mae_regime"] = "higher_error_than_training"
            elif mr <= 0.7:
                flags["mae_regime"] = "easier_than_training"
            else:
                flags["mae_regime"] = "similar_to_training"

        if flags:
            diag["flags"] = flags

    return diag


def _build_extra_context_from_row(
    row: pd.Series,
    shap_ungrouped: Optional[Dict[str, Any]] = None,
    shap_grouped: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Build extra model + train-data + SHAP context for the LLM from a DF row."""
    ctx: Dict[str, Any] = {}
    train_df: Optional[pd.DataFrame] = None  # keep for schema-based time_series summary

    # Model details (JSON string or dict)
    model_details_col = next((c for c in row.index if c.strip().lower() == "model_details"), None)
    if model_details_col:
        ctx["model_details"] = _safe_parse_json_field(row[model_details_col])

    # Train outputs JSON (Trained_Data / model_train_outputs)
    train_out_col = next(
        (c for c in row.index if c.strip().lower() in ("trained_data", "model_train_outputs")),
        None
    )
    if train_out_col:
        parsed_train = _safe_parse_json_field(row[train_out_col])
        ctx["model_train_outputs"] = parsed_train
        try:
            train_df = _trained_data_to_df(parsed_train)
        except Exception as e:
            ctx["model_train_outputs_error"] = f"{type(e).__name__}: {e}"

    # Test outputs JSON (Actuals = y_test, Predictions = y_pred)
    outputs_col = next((c for c in row.index if c.strip().lower() == "model_outputs"), None)
    if XAI_INCLUDE_MODEL_OUTPUTS and outputs_col:
        parsed_outputs = _safe_parse_json_field(row[outputs_col])
        ctx["model_outputs"] = parsed_outputs
        try:
            tbl, summary = _summarize_model_outputs(parsed_outputs)
            if tbl:
                ctx["model_outputs_table_markdown"] = tbl
            if summary:
                ctx["model_outputs_summary"] = summary
        except Exception as e:
            ctx["model_outputs_error"] = f"{type(e).__name__}: {e}"
    # else: Model_Outputs present but intentionally excluded from XAI context

    # Target label name (optional)
    target_label_name: Optional[str] = None
    for c in row.index:
        if c.strip().lower() == "target_label":
            try:
                target_label_name = str(row[c]).strip()
            except Exception:
                target_label_name = None
            break

    # X_train / y_train CSVs
    x_train_col = next((c for c in row.index if c.strip().lower() == "x_train"), None)
    y_train_col = next((c for c in row.index if c.strip().lower() == "y_train"), None)

    x_meta: Dict[str, Any] = {}
    y_meta: Dict[str, Any] = {}
    x_table = ""
    y_table = ""

    # X_train
    try:
        if x_train_col and isinstance(row[x_train_col], str) and row[x_train_col].strip():
            x_path = row[x_train_col]
            x_df = pd.read_csv(x_path)
            date_col_x = next((c for c in x_df.columns if c.strip().lower() in ("date","datetime","ds")), x_df.columns[0])
            feature_cols = [c for c in x_df.columns if c != date_col_x]
            x_meta = {
                "path": x_path,
                "n_rows": int(len(x_df)),
                "n_features": int(len(feature_cols)),
                "date_column": date_col_x,
                "feature_preview": feature_cols[:XAI_MAX_TRAIN_FEATURES],
            }
            try:
                dt = pd.to_datetime(x_df[date_col_x])
                x_meta["date_min"] = str(dt.min())
                x_meta["date_max"] = str(dt.max())
            except Exception:
                pass
            cols_for_table = [date_col_x] + feature_cols[:XAI_MAX_TRAIN_FEATURES]
            x_table = _df_to_markdown_table(x_df[cols_for_table], XAI_MAX_TRAIN_ROWS, len(cols_for_table))
    except Exception as e:
        x_meta = {"error": f"{type(e).__name__}: {e}"}

    # y_train
    try:
        if y_train_col and isinstance(row[y_train_col], str) and row[y_train_col].strip():
            y_path = row[y_train_col]
            y_df = pd.read_csv(y_path)
            date_col_y = next((c for c in y_df.columns if c.strip().lower() in ("date","datetime","ds")), y_df.columns[0])
            if target_label_name and target_label_name in y_df.columns:
                target_col = target_label_name
            else:
                target_col = y_df.columns[1] if len(y_df.columns) > 1 else y_df.columns[0]
            y_meta = {
                "path": y_path,
                "n_rows": int(len(y_df)),
                "target_column": target_col,
                "date_column": date_col_y,
            }
            try:
                dt = pd.to_datetime(y_df[date_col_y])
                y_meta["date_min"] = str(dt.min())
                y_meta["date_max"] = str(dt.max())
            except Exception:
                pass
            y_table = _df_to_markdown_table(y_df[[date_col_y, target_col]], XAI_MAX_TRAIN_ROWS, 2)
    except Exception as e:
        y_meta = {"error": f"{type(e).__name__}: {e}"}

    if x_meta or y_meta:
        ctx["train_data"] = {
            "x_train": x_meta,
            "y_train": y_meta,
            "x_train_table_markdown": x_table,
            "y_train_table_markdown": y_table,
        }

    # --- Optional schema-driven compressed summaries for selected columns ---
    if XAI_CONTEXT_SCHEMA:
        # Model_Details as 'object'
        if "model_details" in ctx:
            md_schema = _apply_context_schema("Model_Details", raw_value=ctx["model_details"])
            if md_schema is not None:
                ctx["model_details_schema"] = md_schema

        # Trained_Data as 'time_series' (if schema defines it)
        if train_df is not None:
            td_schema = _apply_context_schema("Trained_Data", df_value=train_df)
            if td_schema is not None:
                ctx["train_data_schema"] = td_schema

        # Model_Outputs as 'time_series' (if present + schema defines it)
        if "model_outputs" in ctx:
            try:
                mo_df = _model_outputs_to_df(ctx["model_outputs"])
            except Exception:
                mo_df = None
            if mo_df is not None:
                mo_schema = _apply_context_schema("Model_Outputs", df_value=mo_df)
                if mo_schema is not None:
                    ctx["model_outputs_schema"] = mo_schema

    try:
        if "model_details" in ctx:
            reg_summary = _build_regression_summary(ctx["model_details"])
            if reg_summary is not None:
                ctx["regression_summary"] = reg_summary

        mos = ctx.get("model_outputs_summary")
        if "model_details" in ctx or mos:
            fit_diag = _build_fit_diagnostics(ctx.get("model_details"), mos)
            if fit_diag is not None:
                ctx["fit_diagnostics"] = fit_diag
    except Exception as e:
        ctx["fit_diagnostics_error"] = f"{type(e).__name__}: {e}"

    # --- SHAP context filtered by Date ---
    row_date_str = _normalize_row_date(row)

    shap_ctx: Dict[str, Any] = {}

    # SHAP_Weekly_Ungrouped: { "YYYY-MM-DD": {Feature: value, ...}, ... }
    if shap_ungrouped and isinstance(shap_ungrouped, dict) and row_date_str:
        per_date = shap_ungrouped.get(row_date_str)
        if per_date is not None:
            shap_ctx["shap_weekly_ungrouped_for_date"] = {
                "date": row_date_str,
                "feature_shap": per_date
            }

    # SHAP_Weekly_Grouped: { "0": {"Group":..., "Date":..., "Grp_contri"/"grp_contri":...}, ... }
    if shap_grouped and isinstance(shap_grouped, dict) and row_date_str:
        grouped_list: List[Dict[str, Any]] = []
        for v in shap_grouped.values():
            if not isinstance(v, dict):
                continue
            dval = v.get("Date")
            if not dval:
                continue
            try:
                dnorm = _normalize_row_date(pd.Series({"Date": dval}))
            except Exception:
                dnorm = str(dval)[:10]
            if dnorm == row_date_str:
                contrib = v.get("Grp_contri", v.get("grp_contri"))
                grouped_list.append({
                    "Group": v.get("Group"),
                    "Date": row_date_str,
                    "Grp_contri": contrib
                })
        if grouped_list:
            shap_ctx["shap_weekly_grouped_for_date"] = {
                "date": row_date_str,
                "groups": grouped_list
            }

    if shap_ctx:
        ctx["shap_context"] = shap_ctx

    return ctx

def build_llm_payload(
    monthly_flat: Dict[str, Any],
    groups: List[str],
    months: List[str],
    extra_context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    by_month = {m: [] for m in months}
    for rec in monthly_flat.values():
        ym = rec["Date"][:7]
        by_month[ym].append({"name": rec["Group"], "value": float(rec["Avg_grp_contri"])})

    net_effects = compute_net_effect_for_month(by_month)
    label_policy = build_label_policy_from_monthly(monthly_flat)

    months_list = [{"month": m, "groups": by_month[m], "net_effect": net_effects[m]} for m in months]
    response_contract = [{"month": m, "net_effect": net_effects[m]} for m in months]

    group_trends = build_group_trends(months_list)
    month_balance = build_month_balance(months_list)
    prev_map = build_prev_map(months_list)

    return {
        "months": months_list,
        "months_count": len(months),
        "month_order": months,
        "label_policy": label_policy,
        "response_contract": response_contract,
        "context": {
            "group_trends": group_trends,
            "month_balance": month_balance,
            "prev_groups": prev_map,
            "interpretation_guide": (
                "SHAP values are additive explanations relative to a baseline; "
                "they describe contribution signs/magnitudes, not causal effects."
            ),
            # full model + data context (train, outputs, weekly, schemas, etc.)
            "model_context": extra_context or {}
        },
        "notes": {
            "net_effect_definition": "Sum across groups for that month (SHAP additivity).",
            # NEW: make the contract explicit that BOTH SHAP and model_context must be used
            "style": (
                "Analyst-grade, data-grounded narratives that combine SHAP driver patterns "
                "WITH model_context (model details, train window, errors, weekly stability) "
                "whenever that context is available."
            ),
            "llm_is_reasoning_model": bool(IS_REASONING_MODEL)
        }
    }


# ------------- PYDANTIC MODELS ---------------
SCHEMA_VERSION = os.getenv("SCHEMA_VERSION", "1").strip()

class Driver(BaseModel):
    group: str
    direction: Literal["up", "down"]
    magnitude: float
    qualifier: Literal["slightly", "moderately", "significantly"]

class EvidenceSource(BaseModel):
    title: str
    url: str
    outlet: Optional[str] = None
    published: Optional[str] = None

class Evidence(BaseModel):
    driver: str
    why: str
    sources: List[EvidenceSource]

class ContextualReason(BaseModel):
    driver: str
    reason: str
    source_type: Literal["external", "hypothesis", "internal"]
    confidence: Literal["low", "medium", "high"]
    disclaimer: str
    source_url: Optional[str] = None

class MonthAnalysis(BaseModel):
    month: str
    analysis_scratchpad: str = Field(..., description="Step-by-step reasoning comparing SHAP vs Model Error.")
    summary: Optional[str] = Field(None, description="Short headline summary.")
    model_health_context: str = Field(..., description="Statement on reliability/error metrics.")
    integrated_narrative: str = Field(..., description="Narrative combining SHAP and model health.")
    drivers: List[Driver]
    net_effect: float
    net_explanation: str
    
    # Version 2 specific fields (Optional to allow V1 fallback compatibility)
    evidence: Optional[List[Evidence]] = Field(default_factory=list)
    watch_next: Optional[str] = None
    
    # Version 1 specific fields
    contextual_reasons: Optional[List[ContextualReason]] = Field(default_factory=list)

class OverallAnalysis(BaseModel):
    months: List[MonthAnalysis] = Field(..., min_length=4, max_length=4)
    overall_summary: str = Field(..., description="Synthesis of drivers and regime changes.")

class WeeklyAnalysis(BaseModel):
    """Schema for single-week analysis"""
    analysis_scratchpad: str = Field(..., description="Step-by-step reasoning.")
    weekly_narrative: str = Field(..., description="Analyst-grade explanation combining SHAP and model health.")
    model_health_context: str = Field(..., description="Summary of reliability (error, extrapolation).")

# Helper to get the JSON schema as a string for the prompt
def get_pydantic_schema_json(model_class: type[BaseModel]) -> str:
    return json.dumps(model_class.model_json_schema(), indent=2)

# ------------------ SYSTEM PROMPT -----------------
SYSTEM_PROMPT = (
    "You are an XAI narrator for time-series model drivers.\n"
    "Your job is to synthesize the numeric SHAP facts AND the rich model_context\n"
    "into concise, analyst-grade narratives for business readers.\n"
    "\n"
    "INPUT STRUCTURE (inside the JSON payload you receive):\n"
    " • months: four consecutive months, each with {month, groups[{name,value}], net_effect}.\n"
    " • label_policy: thresholds that map absolute magnitudes to qualifiers\n"
    "   {slightly, moderately, significantly}.\n"
    " • context.group_trends: per-driver statistics across the full horizon\n"
    "   (mean contribution, share_positive, sign_changes).\n"
    " • context.month_balance: for each month, |net|/Σ|effects| and a balance_label\n"
    "   (mostly offsetting / mixed / concentrated).\n"
    " • context.prev_groups: previous-month contribution per group, for momentum.\n"
    " • context.model_context: structured information about the model and data, e.g.\n"
    "      - model_details / model_details_schema (model type, hyperparameters, fit metrics),\n"
    "      - regression_summary (for linear models: top coefficients, signs, intercept, train metrics),\n"
    "      - train_data / train_data_schema (sample sizes, ranges, training coverage window),\n"
    "      - model_outputs_summary / model_outputs_schema (test error size, bias, stability),\n"
    "      - fit_diagnostics (train vs test RMSE/MAE, ratios, regime flags),\n"
    "      - time_coverage (train_start/train_end vs explain_start/explain_end, extrapolation flag),\n"
    "      - per_week entries and shap_context (weekly patterns and stability of drivers).\n"
    " • notes.llm_is_reasoning_model: boolean indicating whether you are a reasoning-oriented model;\n"
    "   when true, you are expected to internally plan in steps (scan context, identify anomalies,\n"
    "   connect them to SHAP drivers) before writing the final JSON, but you must output only the\n"
    "   final JSON object.\n"
    "\n"
    "CRITICAL INSTRUCTION ON REASONING & SCRATCHPAD:\n"
    " • You are provided with an 'analysis_scratchpad' field in the JSON schema.\n"
    " • YOU MUST FILL THIS FIELD FIRST for every month.\n"
    " • Use the scratchpad to internally calculate trend directions, identify sign flips,\n"
    "   check if the model error (RMSE/MAE) in 'model_context' undermines the SHAP values,\n"
    "   and decide on the narrative angle.\n"
    " • Do NOT just transcribe inputs. Analyze them.\n"
    "\n"
    "CRITICAL INSTRUCTION ON CONTEXT INTEGRATION:\n"
    " • You are provided with rich 'model_context'. You MUST NOT output a summary that relies only\n"
    "   on SHAP values. For every month, you must also answer: \"Is the model reliable this month\n"
    "   based on the context?\" and encode that answer in 'model_health_context' and\n"
    "   'integrated_narrative'.\n"
    " • When 'fit_diagnostics' is present, explicitly compare train vs test error in the\n"
    "   analysis_scratchpad (e.g. test RMSE vs train RMSE and their ratio) and summarise this\n"
    "   regime (similar / higher / easier) in 'model_health_context'.\n"
    " • If model_outputs_summary indicates high or increasing error (e.g., RMSE or MAE), you must\n"
    "   mention it explicitly.\n"
    " • If 'time_coverage.is_extrapolation' is true, you must explicitly state that the current window\n"
    "   lies beyond the training period and downgrade confidence in the exact magnitudes, while still\n"
    "   discussing directional drivers.\n"
    " • If train_data or train_data_schema suggests the current period is poorly covered by the\n"
    "   training window, or volumes shifted, you must mention potential drift or lower confidence.\n"
    " • Connect these health signals to the SHAP driver story whenever possible.\n"
    "\n"
    "LINEAR REGRESSION-SPECIFIC INSTRUCTIONS (when regression_summary.model_type indicates a linear model):\n"
    " • Use the top_features list (coefficients and signs) to sanity-check SHAP direction.\n"
    "   For each month, especially for the top 2-3 drivers:\n"
    "     - If coefficient sign and SHAP sign agree, treat the signal as structurally consistent.\n"
    "     - If they frequently disagree across months (e.g. positive coefficient but mostly\n"
    "       negative SHAP), highlight this as unstable or regime-shift behaviour.\n"
    " • Use train R^2 and train RMSE/MAE to describe how well the linear model fits its training\n"
    "   data in general, before bringing in test metrics and current-month behaviour.\n"
    "\n"
    "STYLE & BEHAVIOUR:\n"
    " • FAILURE MODE TO AVOID: Do not simply list the drivers in prose. That is useless.\n"
    " • SUCCESS CRITERIA: A good month-level explanation combines driver contributions with\n"
    "   model health and confidence (e.g., \"While Group A pushed the forecast up, error rose,\n"
    "   suggesting the move is less reliable\").\n"
    " • For each month, write roughly 4-5 sentences.\n"
    " • Always start from the SHAP facts: which groups drove the net and in which direction.\n"
    " • THEN, for each month where model_context has relevant information, explicitly weave in at\n"
    "   least one insight from model_context (error, bias, coverage, stability of drivers, etc.).\n"
    " • Use context.group_trends and context.prev_groups to highlight momentum and sign flips\n"
    "   rather than repeating the same template every month.\n"
    " • Vary your wording across months; avoid copy-paste or highly repetitive phrasing.\n"
    " • Make explicit comparisons when something important changes versus prior month\n"
    "   (sign flips, re-ordering of top drivers, sharp shifts in error metrics or volume, etc.).\n"
    " • Treat label_policy and month_balance as heuristics for wording magnitude and balance;\n"
    "   you may override their adjectives if the broader context suggests a different emphasis.\n"
    " • SHAP values are additive contributions relative to a baseline; use cautious language\n"
    "   around causality (\"contributed\", \"was associated with\") rather than strong causal claims.\n"
    "\n"
    "OUTPUT JSON CONTRACT:\n"
    " • Return exactly one JSON object matching the schema below.\n"
    " • Do not wrap the JSON in markdown fences.\n"
    "\n"
    "### JSON SCHEMA:\n"
    f"{get_pydantic_schema_json(OverallAnalysis)}"
)

SYSTEM_PROMPT_WEEKLY = (
    "You are an XAI narrator for a single forecast week in a time-series forecasting model.\n"
    "You receive:\n"
    " - week: {date, groups[{name,value}], net_effect, prev_groups}.\n"
    " - label_policy: magnitude thresholds (slightly/moderately/significantly).\n"
    " - context.model_context: structured info about the model and data for this row\n"
    "   (train_data, model_outputs_summary, regression_summary, fit_diagnostics, shap_context, etc.).\n"
    "You may also receive up to three plot images that show training fit and feature/target behaviour.\n"
    "\n"
    "Your job is to produce a short JSON object analyzing drivers and model health.\n"
    "\n"
    "Guidelines:\n"
    " - Always ground your explanation in the provided 'groups' and 'net_effect' for this week.\n"
    " - Use label_policy to talk about magnitude (slightly/moderately/significantly) but you may\n"
    "   override wording if model_context or the plots show something more severe/benign.\n"
    " - If fit_diagnostics or model_outputs_summary indicate high or worsening error, you MUST\n"
    "   mention it explicitly and lower confidence.\n"
    " - If time_coverage.is_extrapolation is true (when present in context.time_coverage), explicitly say\n"
    "   that this week lies beyond the training window and treat the forecast as lower confidence.\n"
    " - Use cautious language ('contributed', 'associated with') rather than hard causal claims.\n"
    "\n"
    "Output exactly one JSON object matching the schema below.\n"
    "No markdown, no extra keys, no free-text outside the JSON object.\n"
    "\n"
    "### JSON SCHEMA:\n"
    f"{get_pydantic_schema_json(WeeklyAnalysis)}"
)

def build_context_schema_discovery_prompt(column_name: str, sample_values: List[Any]) -> str:
    """
    Helper: build a prompt for a reasoning LLM to design a compact
    xai_context_schema.json entry for a given logical column
    (e.g. "Model_Details" or "Trained_Data").

    This is now used at runtime by the pipeline when XAI_CONTEXT_SCHEMA
    is missing a definition for that column. The caller is responsible
    for:
      - Supplying a small list of representative raw values
        (typically just the first non-null row from the main DataFrame),
      - Sending the prompt to an LLM (via _call_llm_for_schema),
      - Merging the returned configuration into XAI_CONTEXT_SCHEMA.
    """
    examples_json = json.dumps(sample_values, indent=2, ensure_ascii=False)
    tmpl = f"""
        You are helping design a stable, minimal JSON schema for compressing
        the XAI context column "{column_name}".

        You will be shown a few representative raw values from this column.
        Your job is to propose a JSON configuration that a deterministic
        Python pipeline can use to compress these objects into a small,
        consistent summary.

        ### Requirements

        1. Output **only valid JSON**, no commentary.
        2. The top-level JSON MUST have this structure:

        {{
          "version": "xai_context_v1",
          "columns": {{
            "{column_name}": {{
              "type": "object" | "time_series",
              "fields": [
                {{
                  "name": "...",
                  "json_path": "A.B.C"
                }},
                {{
                  "name": "...",
                  "agg": "len" | "min" | "max" | "mean" | "mae" | "rmse" | "mean_error",
                  "column": "ColName" | null,
                  "columns": ["ActualCol", "PredCol"] | null
                }}
              ]
            }}
          }}
        }}

        3. Interpretation:
        - If type == "object":
            - Each field MUST have: "name" and "json_path".
            - "json_path" is a dotted path into the example JSON, e.g.
              "LearnedParams.n_features_in" or "Hyperparameters.fit_intercept".
        - If type == "time_series":
            - The underlying data will be converted to a pandas.DataFrame.
            - Each field MUST have "name" and "agg".
            - For simple stats ("min", "max", "mean"), give a single "column".
            - For error-based stats ("mae", "rmse", "mean_error"), give "columns"
              as a 2-element list [actual_column, prediction_column].

        4. Constraints:
        - Use at most **8 fields** for this column.
        - Only reference keys/columns that are clearly derivable from the
          examples you see.
        - Do NOT invent semantic categories or free-text explanations.
        - The goal is a compact numeric/meta summary suitable for another
          XAI model to consume.

        ### Example raw values for column "{column_name}"

        ```json
        {examples_json}
        ```

        Now return ONLY the JSON configuration described above.
    """
    return tmpl.strip()


def _ensure_context_schema_from_df(df_in: pd.DataFrame) -> None:
    """
    Ensure XAI_CONTEXT_SCHEMA has entries for the logical columns
    'Model_Details' and 'Trained_Data'.

    If they are missing AND the corresponding data exists in df_in,
    we:
      - Take the first non-null value from that column,
      - Build a schema-discovery prompt,
      - Call the LLM once via _call_llm_for_schema,
      - Merge the returned 'columns'[logical_name] into XAI_CONTEXT_SCHEMA.

    This runs at pipeline startup, before per-row context building, so that
    _apply_context_schema() can immediately use the discovered schema.
    """
    global XAI_CONTEXT_SCHEMA

    if not isinstance(XAI_CONTEXT_SCHEMA, dict) or not XAI_CONTEXT_SCHEMA:
        XAI_CONTEXT_SCHEMA = {"version": "xai_context_v1", "columns": {}}
    cols_cfg = XAI_CONTEXT_SCHEMA.setdefault("columns", {})

    # (logical_name, predicate on df_in column name)
    targets: List[Tuple[str, Any]] = [
        ("Model_Details", lambda c: c.strip().lower() == "model_details"),
        ("Trained_Data",  lambda c: c.strip().lower() in ("trained_data", "model_train_outputs")),
    ]

    for logical_name, predicate in targets:
        # Skip if we already have a schema entry
        if logical_name in cols_cfg:
            continue

        # Find physical column in df_in
        phys_col = next((c for c in df_in.columns if predicate(str(c))), None)
        if phys_col is None:
            continue

        series = df_in[phys_col].dropna()
        if series.empty:
            continue

        # Use ONLY the first non-null raw value as you requested
        raw_val = series.iloc[0]
        sample_value = _safe_parse_json_field(raw_val)
        samples = [sample_value]

        try:
            prompt = build_context_schema_discovery_prompt(logical_name, samples)
        except Exception as e:
            print(f"[WARN] Failed to build schema-discovery prompt for {logical_name}: {e}")
            continue

        try:
            schema_resp, _raw = _call_llm_for_schema(prompt)
        except Exception as e:
            print(f"[WARN] LLM schema discovery failed for {logical_name}: {e}")
            continue

        if not isinstance(schema_resp, dict):
            print(f"[WARN] LLM schema discovery returned non-dict for {logical_name}, skipping.")
            continue

        resp_cols = schema_resp.get("columns")
        if not isinstance(resp_cols, dict):
            print(f"[WARN] LLM schema discovery response missing 'columns' for {logical_name}, skipping.")
            continue

        col_cfg = resp_cols.get(logical_name)
        if not isinstance(col_cfg, dict):
            print(f"[WARN] LLM schema discovery response missing '{logical_name}' entry, skipping.")
            continue

        # Optionally adopt version if ours doesn't have one yet
        if "version" in schema_resp and "version" not in XAI_CONTEXT_SCHEMA:
            XAI_CONTEXT_SCHEMA["version"] = schema_resp["version"]

        cols_cfg[logical_name] = col_cfg
        print(f"[INFO] Learned context schema for {logical_name} from LLM.")

# ----------------- LOW-LEVEL JSON PARSING UTILS ------------------------
def _largest_balanced_json_object(text: str) -> Optional[str]:
    start = None
    depth = 0
    best = None
    for i, ch in enumerate(text):
        if ch == '{':
            if depth == 0:
                start = i
            depth += 1
        elif ch == '}':
            if depth > 0:
                depth -= 1
                if depth == 0 and start is not None:
                    cand = text[start:i+1]
                    if best is None or len(cand) > len(best):
                        best = cand
    return best

def _quote_unquoted_keys(s: str) -> str:
    return re.sub(r'([{\s,])([A-Za-z_][A-Za-z0-9_\-]*)\s*:', r'\1"\2":', s)

def _fix_constants(s: str) -> str:
    s = re.sub(r'\bNaN\b', 'null', s, flags=re.IGNORECASE)
    s = re.sub(r'\bInfinity\b', '1e9999', s, flags=re.IGNORECASE)
    s = re.sub(r'\b-Inf(inity)?\b', '-1e9999', s, flags=re.IGNORECASE)
    return s

def _normalize_quotes(s: str) -> str:
    s = s.replace("\u201c", '"').replace("\u201d", '"').replace("\u201e", '"').replace("\u201f", '"')
    s = s.replace("\u2018", "'").replace("\u2019", "'")
    return s

def _strip_fences_and_garbage(s: str) -> str:
    s = s.strip().lstrip("\ufeff")
    if s.startswith("```"):
        blocks = [b.strip() for b in s.split("```") if b.strip()]
        for b in blocks:
            if b.startswith("{") or b.lower().startswith("json"):
                s = b
                break
    if s.lower().startswith("json"):
        s = s[4:].strip()
    if not s.startswith("{") or not s.endswith("}"):
        maybe = _largest_balanced_json_object(s)
        if maybe:
            s = maybe
    return s

def _attempt_json_repair(text: str) -> dict:
    """
    Best-effort JSON repair.

    Priority:
    1) direct json.loads on cleaned string
    2) json_repair (if available)
    3) dirtyjson (if available)
    4) legacy regex-based heuristics (_quote_unquoted_keys, etc.)
    """
    if not text:
        raise ValueError("Empty content; nothing to repair")

    # Clean fences / quotes, but don't over-mangle the content
    t = _strip_fences_and_garbage(text)
    t = _normalize_quotes(t)

    # 1) Try straight JSON parse first (if the model behaved)
    try:
        return json.loads(t)
    except Exception:
        pass

    # 2) Prefer json_repair if installed
    if _repair_json_lib is not None:
        try:
            repaired = _repair_json_lib(t)
            return json.loads(repaired)
        except Exception:
            pass

    # 3) Try dirtyjson as another robust parser
    if _dirtyjson_loads is not None:
        try:
            return _dirtyjson_loads(t)
        except Exception:
            pass

    # 4) Fall back to legacy regex-based repair (previous behaviour)
    t = re.sub(r",(\s*[}\]])", r"\1", t)
    t = _quote_unquoted_keys(t)
    t = _fix_constants(t)
    try:
        return json.loads(t)
    except Exception:
        pass

    maybe = _largest_balanced_json_object(t)
    if maybe:
        try:
            return json.loads(maybe)
        except Exception:
            pass

    # If all attempts fail, raise a clear error instead of a bare re-raise
    raise ValueError("Unable to repair JSON into a valid object")

# ----------------- LLM CALLS (DATABRICKS) ------------------------
def _get_databricks_client_and_model() -> Tuple[Any, str]:
    if _OpenAI is None:
        raise RuntimeError("openai package not available; pip install openai>=1.0.0")

    token = DATABRICKS_TOKEN
    if not token:
        try:
            token = (
                dbutils.notebook.entry_point.getDbutils()  # type: ignore[name-defined]
                .notebook()
                .getContext()
                .apiToken()
                .getOrElse(None)
            )
        except Exception:
            token = ""

    if not token:
        raise RuntimeError("Missing DATABRICKS_TOKEN and could not obtain token from dbutils.")

    base_url = DATABRICKS_BASE_URL
    if not base_url:
        raise RuntimeError("DATABRICKS_BASE_URL is not set.")
    if not base_url.endswith("/serving-endpoints"):
        base_url = base_url.rstrip("/") + "/serving-endpoints"

    client = _OpenAI(api_key=token, base_url=base_url)
    model_used = DATABRICKS_MODEL
    return client, model_used

def call_databricks_llm(payload: Dict[str, Any], images: Optional[list[str]] = None) -> tuple[dict, str]:
    client, model_used = _get_databricks_client_and_model()
    reasoning = _is_reasoning_model_name(model_used)

    def _mk_messages_for(
        contract_payload: Dict[str, Any],
        schema: bool,
        images: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        if schema:
            text = json.dumps(
                {"facts": contract_payload, "contract": contract_payload["response_contract"]},
                separators=(",", ":"),
            )
        else:
            text = json.dumps(contract_payload, separators=(",", ":"))

        # If images are provided, build multimodal content
        if images:
            user_content: Any = [{"type": "text", "text": text}]
            for url in images:
                if not url:
                    continue
                user_content.append(
                    {
                        "type": "image_url",
                        "image_url": {"url": url},
                    }
                )
        else:
            user_content = text

        return [
            {
                "role": "system",
                "content": SYSTEM_PROMPT + ("" if schema else "\nReturn ONE JSON object only."),
            },
            {
                "role": "user",
                "content": user_content,
            },
        ]

    start = time.time()
    kwargs = {
        "model": model_used,
        "messages": _mk_messages_for(payload, schema=False, images=images),
        "response_format": {"type": "json_object"},
    }

    if not reasoning:
        kwargs.update({"temperature": TEMPERATURE, "top_p": TOP_P, "seed": SEED})

    resp = client.chat.completions.create(**kwargs)
    raw = (resp.choices[0].message.content or "").strip()
    try:
        js = json.loads(raw)
    except Exception:
        js = _attempt_json_repair(raw)
    dur = time.time() - start
    ptok = getattr(resp, "usage", None).prompt_tokens if getattr(resp, "usage", None) else _estimate_tokens(SYSTEM_PROMPT)
    otok = getattr(resp, "usage", None).completion_tokens if getattr(resp, "usage", None) else _estimate_tokens(raw)
    LAST_CALL_METRICS.update({
        "backend": "databricks",
        "model": model_used,
        "schema": False,
        "duration_sec": round(dur,3),
        "prompt_tokens_est": ptok,
        "output_tokens_est": otok,
        "tps_est": round((otok / dur) if dur>0 else 0.0, 2),
        "is_reasoning_model": reasoning,
    })
    LLM_METRICS_LOG.append({**LAST_CALL_METRICS})
    return js, raw

def call_databricks_llm_weekly(payload: Dict[str, Any], images: Optional[List[str]] = None) -> tuple[dict, str]:
    """
    Lightweight Databricks LLM call for a single-week XAI explanation.

    Uses SYSTEM_PROMPT_WEEKLY and expects a small JSON object with:
      {analysis_scratchpad, weekly_narrative, model_health_context}
    """
    client, model_used = _get_databricks_client_and_model()
    reasoning = _is_reasoning_model_name(model_used)

    def _mk_messages_for_weekly(
        contract_payload: Dict[str, Any],
        images: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        text = json.dumps(contract_payload, separators=(",", ":"))

        if images:
            user_content: Any = [{"type": "text", "text": text}]
            for url in images:
                if not url:
                    continue
                user_content.append(
                    {
                        "type": "image_url",
                            "image_url": {"url": url},
                    }
                )
        else:
            user_content = text

        return [
            {
                "role": "system",
                "content": SYSTEM_PROMPT_WEEKLY,
            },
            {
                "role": "user",
                "content": user_content,
            },
        ]

    start = time.time()
    kwargs: Dict[str, Any] = {
        "model": model_used,
        "messages": _mk_messages_for_weekly(payload, images=images),
        "response_format": {"type": "json_object"},
    }
    if not reasoning:
        kwargs.update({"temperature": TEMPERATURE, "top_p": TOP_P, "seed": SEED})

    resp = client.chat.completions.create(**kwargs)
    raw = (resp.choices[0].message.content or "").strip()

    try:
        js = json.loads(raw)
    except Exception:
        js = _attempt_json_repair(raw)

    dur = time.time() - start
    usage = getattr(resp, "usage", None)
    ptok = usage.prompt_tokens if usage else _estimate_tokens(SYSTEM_PROMPT_WEEKLY)
    otok = usage.completion_tokens if usage else _estimate_tokens(raw)

    LAST_CALL_METRICS.update({
        "backend": "databricks",
        "model": model_used,
        "schema": False,
        "duration_sec": round(dur, 3),
        "prompt_tokens_est": ptok,
        "output_tokens_est": otok,
        "tps_est": round((otok / dur) if dur > 0 else 0.0, 2),
        "is_reasoning_model": reasoning,
        "call_kind": "weekly_xai",
    })
    LLM_METRICS_LOG.append({**LAST_CALL_METRICS})
    return js, raw


def _call_llm_for_schema(prompt: str) -> tuple[dict, str]:
    """
    Lightweight LLM call for context-schema discovery.

    - Sends a single text prompt (built by build_context_schema_discovery_prompt)
      to the Databricks endpoint.
    - Expects ONE JSON object as output, matching the contract described
      in the prompt.
    - Uses response_format={"type": "json_object"} to bias the model towards
      well-formed JSON, then falls back to _attempt_json_repair if needed.
    """
    client, model_used = _get_databricks_client_and_model()
    reasoning = _is_reasoning_model_name(model_used)

    messages = [
        {
            "role": "system",
            "content": (
                "You are a strict JSON config generator. "
                "The user will ask you to design a small JSON configuration. "
                "You MUST reply with exactly one JSON object and nothing else. "
                "No markdown fences. No prose."
            ),
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]

    start = time.time()
    kwargs: Dict[str, Any] = {
        "model": model_used,
        "messages": messages,
        "response_format": {"type": "json_object"},
    }
    if not reasoning:
        kwargs.update({"temperature": TEMPERATURE, "top_p": TOP_P, "seed": SEED})

    resp = client.chat.completions.create(**kwargs)
    raw = (resp.choices[0].message.content or "").strip()

    try:
        js = json.loads(raw)
    except Exception:
        js = _attempt_json_repair(raw)

    dur = time.time() - start
    usage = getattr(resp, "usage", None)
    ptok = usage.prompt_tokens if usage else _estimate_tokens(prompt)
    otok = usage.completion_tokens if usage else _estimate_tokens(raw)

    # Mark this in metrics as a schema-discovery style call
    LAST_CALL_METRICS.update({
        "backend": "databricks",
        "model": model_used,
        "schema": True,
        "duration_sec": round(dur, 3),
        "prompt_tokens_est": ptok,
        "output_tokens_est": otok,
        "tps_est": round((otok / dur) if dur > 0 else 0.0, 2),
        "is_reasoning_model": reasoning,
        "call_kind": "context_schema_discovery",
    })
    LLM_METRICS_LOG.append({**LAST_CALL_METRICS})
    return js, raw

# ----------------- ROUTER -------------------
def call_llm(payload: Dict[str, Any]) -> tuple[dict, str]:
    # Only Databricks backend is supported
    return call_databricks_llm(payload)

# ----------------- METRICS HELPERS -------------------
def compute_net_effect_for_month(by_month: Dict[str, List[Dict[str, float]]]) -> Dict[str, float]:
    out = {}
    for ym, items in by_month.items():
        s = sum(float(it["value"]) for it in items)
        out[ym] = round(s, 2)
    return out

def _qualifier_for(value: float, policy: Dict[str, Dict[str, float]]) -> str:
    a = abs(value)
    lo = policy["slightly"]["lt"]
    mid_ge = policy["moderately"]["ge"]; mid_lt = policy["moderately"]["lt"]
    if a < lo: return "slightly"
    if a >= mid_ge and a < mid_lt: return "moderately"
    return "significantly"

def _extrema(groups: List[Dict[str, float]]):
    pos = [g for g in groups if g["value"] > 0]
    neg = [g for g in groups if g["value"] < 0]
    top_pos = max(pos, key=lambda x: x["value"]) if pos else None
    top_neg = min(neg, key=lambda x: x["value"]) if neg else None
    return top_pos, top_neg

def _rb_summarize(month_rec: Dict[str, Any], policy: Dict[str, Any], prev_values: Optional[Dict[str, float]] = None) -> Tuple[str, List[Dict[str, Any]], str]:
    groups = month_rec.get("groups", [])
    net = float(month_rec["net_effect"])
    prev_values = prev_values or {}

    sorted_groups = sorted(groups, key=lambda g: abs(g["value"]), reverse=True)
    topk = sorted_groups[:max(2, XAI_MAX_DRIVERS)]

    sum_abs = sum(abs(g["value"]) for g in groups) or 0.0
    balance_ratio = abs(net) / sum_abs if sum_abs else 0.0
    if balance_ratio < 0.30: balance_label = "mostly offsetting"
    elif balance_ratio < 0.60: balance_label = "mixed"
    else: balance_label = "concentrated"

    drivers, parts = [], []

    def delta_str(name: str, cur_val: float) -> str:
        if name in prev_values:
            d = cur_val - prev_values[name]
            if abs(d) >= 0.05:
                return f" vs prior month {d:+.2f}"
        return ""

    for item in topk:
        name = item["name"]; val = float(item["value"])
        q = _qualifier_for(val, policy)
        if val >= 0:
            drivers.append({"group": name, "direction": "up", "magnitude": round(val, 2), "qualifier": q})
            parts.append(f'{name} contributed {q} positive impact (+{val:.2f}{delta_str(name, val)})')
        else:
            drivers.append({"group": name, "direction": "down", "magnitude": round(abs(val), 2), "qualifier": q})
            parts.append(f'{name} contributed {q} negative impact (-{abs(val):.2f}{delta_str(name, val)})')

    top_pos, top_neg = _extrema(groups)
    if top_pos or top_neg:
        tail = []
        if top_pos: tail.append(f"strongest up: {top_pos['name']} (+{top_pos['value']:.2f})")
        if top_neg: tail.append(f"strongest down: {top_neg['name']} ({top_neg['value']:.2f})")
        if tail:
            parts.append("; ".join(tail))

    if not parts:
        summary = "No dominant drivers; movements were small and largely offsetting."
        drivers = []
    else:
        summary = "; ".join(parts)
        if len(summary) > 240:
            mid = len(summary)//2
            cut = summary.rfind("; ", 0, mid)
            if cut != -1:
                summary = summary[:cut+1] + " " + summary[cut+2:] + "."
            else:
                summary += "."
        else:
            summary += "."

    if sum_abs > 0:
        net_expl = f'Net effect {net:+.2f} = sum of monthly SHAP contributions. Profile: {balance_label} (|net|/Σ|effects| = {balance_ratio:.2f}).'
    else:
        net_expl = "Net effect +0.00 = no material contributions this month."

    summary += f' Overall monthly impact: {net:+.2f} ({balance_label}).'
    return summary, drivers, net_expl

def _compose_overall_summary(payload: Dict[str, Any], months_out: List[Dict[str, Any]]) -> str:
    months_order: List[str] = payload["month_order"]
    month_balance: Dict[str, Any] = payload["context"]["month_balance"]
    facts_by_month = {m["month"]: m for m in payload["months"]}
    nets = {m: round(float(facts_by_month[m]["net_effect"]), 2) for m in months_order}
    def _lab(m): return (month_balance[m]["balance_label"], month_balance[m]["balance_ratio"])
    offsetting = [(m, nets[m], _lab(m)[1]) for m in months_order if _lab(m)[0] == "mostly offsetting"]
    mixed      = [(m, nets[m], _lab(m)[1]) for m in months_order if _lab(m)[0] == "mixed"]
    conc_pos   = [(m, nets[m], _lab(m)[1]) for m in months_order if _lab(m)[0] == "concentrated" and nets[m] > 0]
    conc_neg   = [(m, nets[m], _lab(m)[1]) for m in months_order if _lab(m)[0] == "concentrated" and nets[m] < 0]

    groups = {}
    for m in months_order:
        for g in facts_by_month[m]["groups"]:
            name = g["name"]; val = float(g["value"])
            groups.setdefault(name, {"vals": [], "months": []})
            groups[name]["vals"].append(val)
            groups[name]["months"].append(m)

    group_stats = {}
    for name, info in groups.items():
        vals = info["vals"]; ms = info["months"]
        total = round(sum(vals), 2)
        sign_flips = sum(1 for i in range(1, len(vals)) if (vals[i] > 0) != (vals[i-1] > 0))
        max_up = max(vals) if vals else 0.0
        max_up_idx = vals.index(max_up) if vals else 0
        max_down = min(vals) if vals else 0.0
        max_down_idx = vals.index(max_down) if vals else 0
        rng = round((max_up - max_down), 2)
        mom = round(vals[-1] - vals[0], 2) if len(vals) >= 2 else 0.0
        group_stats[name] = {
            "total": total,
            "sign_flips": sign_flips,
            "max_up": round(max_up, 2),
            "max_up_m": ms[max_up_idx],
            "max_down": round(max_down, 2),
            "max_down_m": ms[max_down_idx],
            "range": abs(rng),
            "momentum": mom,
            "last": round(vals[-1], 2),
            "first": round(vals[0], 2),
        }

    sorted_total = sorted(group_stats.items(), key=lambda kv: kv[1]["total"], reverse=True)
    top_tail = next((kv for kv in sorted_total if kv[1]["total"] > 0), None)
    top_head = next((kv for kv in sorted(group_stats.items(), key=lambda kv: kv[1]["total"]) if kv[1]["total"] < 0), None)
    most_vol = max(group_stats.items(), key=lambda kv: (kv[1]["sign_flips"], kv[1]["range"])) if group_stats else None

    last_m = months_order[-1]
    last_vals = []
    for name, st in group_stats.items():
        try:
            idx = groups[name]["months"].index(last_m)
            last_vals.append((name, round(groups[name]["vals"][idx], 2)))
        except Exception:
            pass
    last_ups  = sorted([(n,v) for (n,v) in last_vals if v > 0], key=lambda x: x[1], reverse=True)[:2]
    last_down = sorted([(n,abs(v)) for (n,v) in last_vals if v < 0], key=lambda x: x[1], reverse=True)[:2]

    def ml(m): return datetime.strptime(m + "-01", "%Y-%m-%d").strftime("%b %Y")
    def mfmt(triplets):
        return ", ".join(f"{ml(m)} ({nets[m]:+,.2f}; r={month_balance[m]['balance_ratio']:.2f})" for m,_,_ in triplets)

    lines = []
    window = f"{ml(months_order[0])}–{ml(months_order[-1])}"
    head_bits = []
    if offsetting: head_bits.append(f"offsetting in {mfmt(offsetting)}")
    if mixed:      head_bits.append(f"mixed in {mfmt(mixed)}")
    if conc_pos:   head_bits.append(f"concentrated up in {mfmt(conc_pos)}")
    if conc_neg:   head_bits.append(f"concentrated down in {mfmt(conc_neg)}")
    if head_bits:
        lines.append(f"Net picture ({window}): " + "; ".join(head_bits) + ".")
    else:
        lines.append(f"Net picture ({window}): monthly nets were modest without clear concentration.")

    story = []
    if top_tail:
        n, st = top_tail
        story.append(f"Top tailwind: {n} (total {st['total']:+.2f}; peak {st['max_up']:+.2f} in {ml(st['max_up_m'])}).")
    if top_head:
        n, st = top_head
        story.append(f"Largest headwind: {n} (total {st['total']:+.2f}; trough {st['max_down']:+.2f} in {ml(st['max_down_m'])}).")
    if most_vol:
        n, st = most_vol
        story.append(f"Most volatile: {n} (sign flips {st['sign_flips']}, range ±{st['range']:.2f}).")
    if last_ups:
        story.append("Momentum into last month: " + ", ".join(f"{n} (+{v:.2f})" for n,v in last_ups) + ".")
    if last_down:
        story.append("Last-month headwinds: " + ", ".join(f"{n} (-{v:.2f})" for n,v in last_down) + ".")
    if story:
        lines.append("Driver storyline: " + " ".join(story))

    ratios = ", ".join(f"{ml(m)} {month_balance[m]['balance_ratio']:.2f}" for m in months_order)
    lines.append(f"Mix & concentration (|net|/Σ|effects|): {ratios}.")

    watch_bits = []
    if most_vol:
        n, _ = most_vol
        watch_bits.append(f"{n} directionality (another sign flip could dominate the net)")
    if top_head:
        n, _ = top_head
        watch_bits.append(f"persistence of {n} headwinds")
    if not watch_bits:
        watch_bits.append("shifts in drivers with largest absolute monthly magnitudes")
    lines.append("What to watch next: " + "; ".join(watch_bits) + ".")

    lines.append("Note: SHAP is additive relative to a baseline; it explains contributions, not causality.")
    return "\n".join(lines)

def normalize_llm_out(llm_out: Dict[str, Any], payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalizes LLM output using Pydantic validation with Rule-Based fallback.
    """
    policy = payload["label_policy"]
    want_order = payload["month_order"]
    by_month = {m["month"]: m for m in payload["months"]}
    prev_map = payload.get("context", {}).get("prev_groups", {})
    
    # 1. Attempt to validate the raw dictionary using Pydantic
    try:
        # We try to parse the entire object at once
        analysis = OverallAnalysis.model_validate(llm_out)
        return analysis.model_dump()
        
    except ValidationError as e:
        # 2. Hybrid Fallback: If validation fails, we reconstruct the object using 
        #    what valid data we have + Rule-Based (RB) fillers.
        print(f"[WARN] Pydantic validation failed: {e}. Falling back to hybrid RB generation.")
        
        months_out_objs = []
        raw_months = llm_out.get("months", [])
        if not isinstance(raw_months, list): 
            raw_months = []
        
        raw_months_map = {
            m.get("month"): m for m in raw_months if isinstance(m, dict) and m.get("month")
        }

        for ym in want_order:
            existing = raw_months_map.get(ym, {})
            
            scratchpad = existing.get("analysis_scratchpad", "RB Fallback: Validation failed.")
            summary = existing.get("summary")
            health = existing.get("model_health_context", "")
            narrative = existing.get("integrated_narrative")
            drivers_raw = existing.get("drivers")
            net_expl = existing.get("net_explanation")
            net_val = existing.get("net_effect", by_month[ym]["net_effect"])

            # Check validity of critical fields
            is_summary_ok = isinstance(summary, str) and summary.strip()
            is_drivers_ok = isinstance(drivers_raw, list) and len(drivers_raw) > 0
            is_net_ok = isinstance(net_expl, str) and net_expl.strip()
            is_narrative_ok = isinstance(narrative, str) and narrative.strip()

            # If any critical field is missing/bad, run Rule-Based calculation
            if not (is_summary_ok and is_drivers_ok and is_net_ok):
                facts = {"month": ym, "groups": by_month[ym]["groups"], "net_effect": net_val}
                rb_summ, rb_drv_list, rb_net_expl = _rb_summarize(facts, policy, prev_map.get(ym, {}))
                
                if not is_summary_ok: summary = rb_summ
                if not is_drivers_ok: drivers_raw = rb_drv_list
                if not is_net_ok: net_expl = rb_net_expl

            if not is_narrative_ok:
                narrative = summary or ""

            if not health:
                health = "Model health context unavailable (fallback)."

            # Convert raw drivers dicts to Pydantic Driver objects
            final_drivers = []
            if isinstance(drivers_raw, list):
                for d in drivers_raw:
                    try:
                        final_drivers.append(Driver(**d))
                    except ValidationError:
                        continue
            
            # If Pydantic conversion of drivers failed completely, regenerate RB drivers
            if not final_drivers:
                facts = {"month": ym, "groups": by_month[ym]["groups"], "net_effect": net_val}
                _, rb_drv_list, _ = _rb_summarize(facts, policy, prev_map.get(ym, {}))
                for d in rb_drv_list:
                    final_drivers.append(Driver(**d))

            # Build MonthAnalysis object safely
            # We check if optional lists exist in source before creating objects
            ma = MonthAnalysis(
                month=ym,
                analysis_scratchpad=str(scratchpad),
                summary=summary,
                model_health_context=str(health),
                integrated_narrative=str(narrative),
                drivers=final_drivers,
                net_effect=float(net_val),
                net_explanation=str(net_expl),
                evidence=[Evidence(**e) for e in existing.get("evidence", [])] if "evidence" in existing else [],
                watch_next=existing.get("watch_next"),
                contextual_reasons=[ContextualReason(**c) for c in existing.get("contextual_reasons", [])] if "contextual_reasons" in existing else []
            )
            months_out_objs.append(ma)

        # Handle Overall Summary
        llm_overall = llm_out.get("overall_summary")
        if not (isinstance(llm_overall, str) and len(llm_overall.strip()) > 10):
            # Generate auto summary if LLM failed
            from_dicts = [m.model_dump() for m in months_out_objs] # helper needs dicts
            llm_overall = _compose_overall_summary(payload, from_dicts)

        analysis = OverallAnalysis(
            months=months_out_objs,
            overall_summary=llm_overall
        )

    return analysis.model_dump()
# ---------------- DATA PREPROCESSING ------------------
def _safe_load_weekly_json(x: Any) -> Dict[str, Any]:
    if isinstance(x, dict):
        return x
    if isinstance(x, str):
        try:
            return json.loads(x)
        except Exception:
            if _repair_json_lib is not None:
                try:
                    repaired = _repair_json_lib(x)
                    return json.loads(repaired)
                except Exception:
                    pass
            maybe = _largest_balanced_json_object(x)
            if maybe:
                return json.loads(maybe)
    raise ValueError("Unable to parse Weekly/SHAP_Weekly_Grouped JSON")

def _make_monthly_text(llm_out: Dict[str, Any], payload: Dict[str, Any]) -> str:
    lines = []
    for m in payload["months"]:
        ym = m["month"]
        mo = next((x for x in llm_out["months"] if x["month"] == ym), None)
        if mo:
            label = datetime.strptime(ym + "-01", "%Y-%m-%d").strftime("%b %Y")
            balance = payload["context"]["month_balance"][ym]["balance_label"]
            # Prefer integrated_narrative, fall back to summary
            narrative = mo.get("integrated_narrative") or mo.get("summary", "")
            health = mo.get("model_health_context") or ""
            if health:
                lines.append(
                    f"- {label}: {narrative} Net (sum of SHAP): {mo['net_effect']:+.2f} [{balance}]. "
                    f"Model health: {health}"
                )
            else:
                lines.append(
                    f"- {label}: {narrative} Net (sum of SHAP): {mo['net_effect']:+.2f} [{balance}]"
                )
            evid = mo.get("evidence", [])
            if evid:
                for ev in evid[:2]:
                    srcs = ev.get("sources", [])[:2]
                    if srcs:
                        src_bits = "; ".join(
                            f"{s.get('outlet', '')}: {s.get('title', '')}"
                            for s in srcs if s.get("title")
                        )
                        lines.append(f"  • Evidence ({ev.get('driver', '')}): {ev.get('why', '')}. {src_bits}")
                    else:
                        lines.append(f"  • Evidence ({ev.get('driver', '')}): {ev.get('why', '')}.")
            else:
                cr = mo.get("contextual_reasons", [])
                if cr:
                    for r in cr[:2]:
                        if r.get("source_url"):
                            lines.append(
                                f"  • Context ({r.get('driver', '')}): {r.get('reason', '')} "
                                f"[{r.get('source_url')}]"
                            )
                        else:
                            lines.append(
                                f"  • Context ({r.get('driver', '')}): {r.get('reason', '')}"
                            )
    lines.append("\n=== OVERALL SUMMARY ===")
    lines.append(llm_out.get("overall_summary", ""))
    return "\n".join(lines)

def run_xai_once(
    weekly_flat: Dict[str, Any],
    extra_context: Optional[Dict[str, Any]] = None,
    images: Optional[List[str]] = None,
) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any], str]:

    """
    Aggregate SHAP_Weekly_Grouped to monthly and run the full LLM XAI pipeline.
    Always produces monthly-level summaries + overall summary.
    """
    monthly_flat, groups, months = aggregate_weekly_flat_to_monthly_mean(
        weekly_flat,
        out_months=4
    )
    payload = build_llm_payload(monthly_flat, groups, months, extra_context=extra_context)
    try:
        llm_raw_out, _raw = call_llm(payload, images=images)
        llm_out = normalize_llm_out(llm_raw_out, payload)
    except Exception as e:
        print(f"[ERROR] LLM Pipeline failed completely: {e}")
        llm_out = normalize_llm_out({}, payload)

    xai_text = _make_monthly_text(llm_out, payload)
    return monthly_flat, payload, llm_out, xai_text

def run_xai_weekly_once(
    date_str: str,
    weekly_rec: Dict[str, Any],
    label_policy: Dict[str, Any],
    prev_values: Optional[Dict[str, float]] = None,
    model_context: Optional[Dict[str, Any]] = None,
    time_coverage: Optional[Dict[str, Any]] = None,
    images: Optional[List[str]] = None,
) -> Tuple[str, Dict[str, Any]]:
    """
    Run a single-week XAI LLM call (Option A) with row-specific plots.

    Returns:
      weekly_text: final explanation string for this week.
      raw_json:    raw JSON object returned by the LLM (may be empty on failure).
    """
    if not weekly_rec:
        return "", {}

    payload: Dict[str, Any] = {
        "week": {
            "date": date_str,
            "groups": weekly_rec.get("groups", []),
            "net_effect": weekly_rec.get("net_effect", 0.0),
            "prev_groups": prev_values or {},
        },
        "label_policy": label_policy,
        "context": {
            "model_context": model_context or {},
        },
        "notes": {
            "llm_is_reasoning_model": bool(IS_REASONING_MODEL),
        },
    }
    if time_coverage:
        payload["context"]["time_coverage"] = time_coverage

    try:
        llm_raw_out, _raw = call_databricks_llm_weekly(payload, images=images)
        # Pydantic parsing
        analysis = WeeklyAnalysis.model_validate(llm_raw_out)
        
        analysis_scratch = analysis.analysis_scratchpad
        weekly_narrative = analysis.weekly_narrative
        model_health = analysis.model_health_context
        
        # Return full dict for logging/metrics
        llm_out_dict = analysis.model_dump()

    except Exception:
        # Fallback if Pydantic fails
        llm_out_dict = {}
        weekly_narrative = ""
        model_health = ""

    # Rule-Based Fallback if LLM didn't give a narrative
    if not weekly_narrative:
        facts = {
            "month": date_str,  # reused field name; semantically it's a week here
            "groups": weekly_rec.get("groups", []),
            "net_effect": weekly_rec.get("net_effect", 0.0),
        }
        rb_summary, _rb_drivers, _rb_net_expl = _rb_summarize(facts, label_policy, prev_values or {})
        weekly_narrative = rb_summary

    if model_health:
        weekly_text = f"{weekly_narrative} Model health: {model_health}"
    else:
        weekly_text = weekly_narrative

    return weekly_text, llm_out_dict

def _build_plot_images_from_context(base_row_ctx: Dict[str, Any]) -> List[str]:
    """
    Build a small set of plot images (as data URLs) from the base_row_ctx:
    - Trained_Data: Actuals vs Fitted
    - X_train: feature scatter vs time
    - y_train: target series

    These are used as multimodal image inputs, not embedded in JSON.
    """
    images: List[str] = []

    # 1) Trained_Data (model_train_outputs)
    try:
        mto = base_row_ctx.get("model_train_outputs")
        if mto is not None:
            td_df = _trained_data_to_df(mto)
            if td_df is not None:
                url = _plot_trained_data_data_url(td_df)
                if url:
                    images.append(url)
    except Exception:
        pass

    train_data = base_row_ctx.get("train_data") or {}

    # 2) X_train
    try:
        x_meta = train_data.get("x_train") or {}
        x_path = x_meta.get("path")
        if x_path:
            x_df = pd.read_csv(x_path)
            date_col_x = x_meta.get("date_column")
            if not date_col_x or date_col_x not in x_df.columns:
                date_col_x = next(
                    (c for c in x_df.columns if c.strip().lower() in ("date", "datetime", "ds")),
                    x_df.columns[0],
                )
            feature_cols = [c for c in x_df.columns if c != date_col_x]
            url = _plot_x_train_scatter_data_url(x_df, date_col_x, feature_cols)
            if url:
                images.append(url)
    except Exception:
        pass

    # 3) y_train
    try:
        y_meta = train_data.get("y_train") or {}
        y_path = y_meta.get("path")
        if y_path:
            y_df = pd.read_csv(y_path)
            date_col_y = y_meta.get("date_column")
            target_col = y_meta.get("target_column")
            if not date_col_y or date_col_y not in y_df.columns:
                date_col_y = next(
                    (c for c in y_df.columns if c.strip().lower() in ("date", "datetime", "ds")),
                    y_df.columns[0],
                )
            if not target_col or target_col not in y_df.columns:
                target_col = y_df.columns[1] if len(y_df.columns) > 1 else y_df.columns[0]
            url = _plot_y_train_data_url(y_df, date_col_y, target_col)
            if url:
                images.append(url)
    except Exception:
        pass

    # Hard cap to avoid going insane with context size
    return images[:3]


def run_pipeline_from_df(df_in: pd.DataFrame) -> Optional[pd.DataFrame]:
    # Weekly/grouped SHAP column (still expect 'SHAP_Weekly_Grouped')
    weekly_col = next(
        (c for c in df_in.columns if c.strip().lower() == "shap_weekly_grouped"),
        None
    )
    if weekly_col is None:
        raise ValueError("Input DataFrame must contain 'SHAP_Weekly_Grouped'")

    # NEW: main indexing columns for this version
    date_col = next((c for c in df_in.columns if c.strip().lower() == "date"), None)
    fw_col = next((c for c in df_in.columns if c.strip().lower() == "forecast_week"), None)
    if date_col is None or fw_col is None:
        raise ValueError("Input DataFrame must contain 'Date' and 'Forecast_Week' columns.")

    # Global SHAP JSONs (only last row has entries, per your note)
    shap_ungrouped_col = next((c for c in df_in.columns if c.strip().lower() == "shap_weekly_ungrouped"), None)
    shap_grouped_col   = next((c for c in df_in.columns if c.strip().lower() == "shap_weekly_grouped"), None)

    shap_ungrouped_global: Optional[Dict[str, Any]] = None
    shap_grouped_global: Optional[Dict[str, Any]] = None

    if shap_ungrouped_col:
        series = df_in[shap_ungrouped_col].dropna()
        if not series.empty:
            shap_ungrouped_global = _safe_parse_json_field(series.iloc[-1])

    if shap_grouped_col:
        series = df_in[shap_grouped_col].dropna()
        if not series.empty:
            shap_grouped_global = _safe_parse_json_field(series.iloc[-1])
            
    _ensure_context_schema_from_df(df_in)

    weekly_series = df_in[weekly_col].dropna()
    if weekly_series.empty:
        raise ValueError(f"Column '{weekly_col}' does not contain any non-null grouped SHAP.")

    # This is the global SHAP_Weekly_Grouped JSON we aggregate to monthly
    weekly_flat_global = _safe_load_weekly_json(weekly_series.iloc[-1])

    # ---------- 1) Build per-row context (X_train, y_train, Model_Outputs, SHAP per date, etc.) ----------
    per_row_ctx_by_date: Dict[str, Dict[str, Any]] = {}

    for _, row in df_in.iterrows():
        row_date_str = _normalize_row_date(row)
        if not row_date_str:
            continue
        extra_ctx = _build_extra_context_from_row(row, shap_ungrouped_global, shap_grouped_global)
        fw_val = row[fw_col] if fw_col in row.index else None
        per_row_ctx_by_date[row_date_str] = {
            "forecast_week": fw_val,
            "model_context": extra_ctx,
        }

    # ---------- 2) Weekly rule-based summaries from grouped SHAP ----------
    weekly_recs = build_weekly_recs_from_grouped(weekly_flat_global)
    weekly_prev_map = build_weekly_prev_map(weekly_recs)

    # Use monthly aggregation only to build magnitude label policy (slightly/moderately/significantly)
    try:
        monthly_flat_for_policy, _groups_policy, _months_policy = aggregate_weekly_flat_to_monthly_mean(
            weekly_flat_global,
            out_months=4
        )
        label_policy = build_label_policy_from_monthly(monthly_flat_for_policy)
    except Exception:
        # Fallback thresholds if aggregation fails for any reason
        label_policy = {
            "slightly":      {"lt": 1.0},
            "moderately":    {"ge": 1.0, "lt": 2.0},
            "significantly": {"ge": 2.0}
        }

    weekly_rb_by_date: Dict[str, Dict[str, Any]] = {}
    for d, rec in weekly_recs.items():
        try:
            facts = {
                "month": d,  # reused field name; semantically it's "week" here
                "groups": rec["groups"],
                "net_effect": rec["net_effect"],
            }
            prev_vals = weekly_prev_map.get(d, {})
            w_summary, w_drivers, w_net_expl = _rb_summarize(facts, label_policy, prev_vals)
        except Exception as e:
            w_summary = f"[RB_ERROR] {type(e).__name__}: {e}"
            w_drivers = []
            w_net_expl = ""
        weekly_rb_by_date[d] = {
            "date": d,
            "summary": w_summary,
            "drivers": w_drivers,
            "net_effect": rec["net_effect"],
            "net_explanation": w_net_expl,
        }

    # ---------- 3) Aggregate weekly + model context for the single LLM run ----------
    extra_context_all: Dict[str, Any] = {}
    dates_sorted = sorted(weekly_recs.keys())
    per_week_ctx_list: List[Dict[str, Any]] = []
    base_row_ctx: Optional[Dict[str, Any]] = None

    for d in dates_sorted:
        row_wrap = per_row_ctx_by_date.get(d, {})
        model_ctx = row_wrap.get("model_context", {}) or {}
        if base_row_ctx is None and model_ctx:
            base_row_ctx = model_ctx

        fw_val = row_wrap.get("forecast_week")
        rb_week = weekly_rb_by_date.get(d, {})

        entry: Dict[str, Any] = {
            "date": d,
            "forecast_week": fw_val,
            "drivers": weekly_recs[d]["groups"],
            "net_effect": weekly_recs[d]["net_effect"],
            "weekly_summary_rb": rb_week.get("summary"),
            "weekly_drivers_rb": rb_week.get("drivers"),
            "weekly_net_explanation_rb": rb_week.get("net_explanation"),
        }

        mo_sum = model_ctx.get("model_outputs_summary")
        if mo_sum:
            entry["model_outputs_summary"] = mo_sum
        shap_ctx = model_ctx.get("shap_context")
        if shap_ctx:
            entry["shap_context"] = shap_ctx

        per_week_ctx_list.append(entry)

        extra_context_all["per_week"] = per_week_ctx_list
        
        # After building per_week_ctx_list and base_row_ctx
    # --- NEW: time coverage context (train window vs explained window) ---
    time_cov: Dict[str, Any] = {}
    if base_row_ctx:
        td = base_row_ctx.get("train_data") or {}
        x_meta = td.get("x_train") or {}
        ts = x_meta.get("date_min")
        te = x_meta.get("date_max")
        if ts:
            time_cov["train_start"] = ts
        if te:
            time_cov["train_end"] = te

    if dates_sorted:
        explain_start = dates_sorted[0]
        explain_end = dates_sorted[-1]
        time_cov["explain_start"] = explain_start
        time_cov["explain_end"] = explain_end

        try:
            if "train_end" in time_cov:
                te_dt = pd.to_datetime(time_cov["train_end"])
                ee_dt = pd.to_datetime(explain_end)
                time_cov["is_extrapolation"] = bool(ee_dt > te_dt)
        except Exception:
            pass

    if time_cov:
        extra_context_all["time_coverage"] = time_cov


    images: List[str] = []

    if base_row_ctx:
        if "model_details" in base_row_ctx:
            extra_context_all["model_details"] = base_row_ctx["model_details"]
        if "train_data" in base_row_ctx:
            extra_context_all["train_data"] = base_row_ctx["train_data"]

        # Build multimodal plot images from the same base_row_ctx
        try:
            images = _build_plot_images_from_context(base_row_ctx)
        except Exception:
            images = []

    # ---------- 4) Single LLM call at monthly level (uses all weeks via extra_context_all + images) ----------
    monthly_flat, payload, llm_out, xai_text = run_xai_once(
        weekly_flat_global,
        extra_context=extra_context_all,
        images=images if images else None,
    )


        # Map month ('YYYY-MM') -> monthly LLM summary text
    month_summary_by_ym: Dict[str, str] = {}
    if isinstance(llm_out, dict):
        months_arr = llm_out.get("months", [])
        if isinstance(months_arr, list):
            for m_obj in months_arr:
                if not isinstance(m_obj, dict):
                    continue
                ym = m_obj.get("month")
                if isinstance(ym, str):
                    # Prefer integrated_narrative, fallback to summary
                    txt = m_obj.get("integrated_narrative") or m_obj.get("summary", "")
                    month_summary_by_ym[ym] = txt


    # Map month ('YYYY-MM') -> last Date ('YYYY-MM-DD') present in df_in
    last_date_by_ym: Dict[str, str] = {}
    try:
        _dates = pd.to_datetime(df_in[date_col])
        _ym_series = _dates.dt.strftime("%Y-%m")
        for ym in _ym_series.unique():
            mask = (_ym_series == ym)
            max_dt = _dates[mask].max()
            if pd.isna(max_dt):
                continue
            last_date_by_ym[ym] = max_dt.strftime("%Y-%m-%d")
    except Exception:
        last_date_by_ym = {}

    # ---------- 5) Build output rows per forecast week ----------
    records: List[Dict[str, Any]] = []
    metrics = {**LAST_CALL_METRICS} if LAST_CALL_METRICS else {}
    llm_model = metrics.get("model", DATABRICKS_MODEL)
    llm_prompt_tokens = metrics.get("prompt_tokens_est")
    llm_output_tokens = metrics.get("output_tokens_est")
    llm_tps = metrics.get("tps_est")
    llm_duration = metrics.get("duration_sec")
    llm_schema_mode = metrics.get("schema")
    llm_is_reasoning = metrics.get("is_reasoning_model", IS_REASONING_MODEL)

    monthly_json_drivers = json.dumps(monthly_flat, separators=(',', ':'), ensure_ascii=False)
    weekly_json_drivers = json.dumps(weekly_flat_global, separators=(',', ':'), ensure_ascii=False)

    for _, row in df_in.iterrows():
        row_date_str = _normalize_row_date(row)
        fw_val = row[fw_col] if fw_col in row.index else None

        # Weekly RB summary (still computed as a safety net)
        rb_week = weekly_rb_by_date.get(row_date_str or "", {})
        weekly_summary = rb_week.get("summary", "")

        # Monthly explanation only on the last forecast week of each month
        monthly_expl = ""
        if row_date_str:
            ym = row_date_str[:7]
            last_for_month = last_date_by_ym.get(ym)
            if last_for_month and row_date_str == last_for_month:
                monthly_expl = month_summary_by_ym.get(ym, "")

        # NEW: per-row weekly LLM with row-specific plots (Option A)
        weekly_llm_text = ""
        weekly_rec = weekly_recs.get(row_date_str or "", {})

        row_ctx_wrap = per_row_ctx_by_date.get(row_date_str or "", {})
        row_model_ctx = row_ctx_wrap.get("model_context") or {}

        row_images: List[str] = []
        if row_model_ctx:
            try:
                row_images = _build_plot_images_from_context(row_model_ctx)
            except Exception:
                row_images = []

        if row_date_str and weekly_rec and (row_model_ctx or row_images):
            try:
                weekly_llm_text, _weekly_llm_json = run_xai_weekly_once(
                    date_str=row_date_str,
                    weekly_rec=weekly_rec,
                    label_policy=label_policy,
                    prev_values=weekly_prev_map.get(row_date_str, {}),
                    model_context=row_model_ctx,
                    time_coverage=time_cov if isinstance(time_cov, dict) else None,
                    images=row_images[:3] if row_images else None,
                )
            except Exception:
                weekly_llm_text = ""

        # Choose primary weekly explanation:
        #  - prefer weekly LLM text if available
        #  - fall back to RB summary if LLM failed
        if weekly_llm_text:
            xai_weekly_expl = weekly_llm_text
        else:
            xai_weekly_expl = weekly_summary

        # Append monthly context (unchanged logic)
        if monthly_expl:
            short_month = monthly_expl.strip()
            if len(short_month) > 240:
                # crude truncation at a sentence boundary if possible
                dot_idx = short_month.find(".", 180)
                if dot_idx != -1:
                    short_month = short_month[:dot_idx + 1]
                else:
                    short_month = short_month[:240] + "..."
            if xai_weekly_expl:
                xai_weekly_expl = f"{xai_weekly_expl} Monthly context: {short_month}"
            else:
                # if everything else failed, at least give a weekly-ish hook into the monthly LLM story
                xai_weekly_expl = f"Weekly pattern aligned with: {short_month}"

        records.append({
            "Date": row_date_str or row.get(date_col),
            "Forecast_Week": fw_val,
            "Weekly_JSON_Drivers": weekly_json_drivers,
            "Monthly_JSON_Drivers": monthly_json_drivers,
            "XAI_JSON_Params": json.dumps(llm_out, separators=(',', ':'), ensure_ascii=False),
            # Weekly-level explanation for this forecast week (LLM weekly + monthly context, RB as fallback)
            "XAI_Explaination": xai_weekly_expl,
            # Monthly-level explanation (only populated on last week of month)
            "XAI_Monthly_Explaination": monthly_expl,
            # Weekly rule-based layer per forecast week
            "XAI_Weekly_JSON_RB": json.dumps(rb_week, separators=(',', ':'), ensure_ascii=False) if rb_week else "",
            "XAI_Weekly_Summary_RB": weekly_summary,
            # LLM metrics (will now reflect the last LLM call made in this run)
            "LLM_Model": llm_model,
            "LLM_Prompt_Tokens": llm_prompt_tokens,
            "LLM_Output_Tokens": llm_output_tokens,
            "LLM_TPS_Est": llm_tps,
            "LLM_Duration_Sec": llm_duration,
            "LLM_Schema_Mode": llm_schema_mode,
            "LLM_Is_Reasoning_Model": llm_is_reasoning,
            # Explicitly record whether Model_Outputs was included in XAI context
            "XAI_Include_Model_Outputs": bool(XAI_INCLUDE_MODEL_OUTPUTS),
        })


    out_df = pd.DataFrame.from_records(records)
    out_df.to_excel(XAI_OUTPUT_XLS, index=False, engine="openpyxl")
    if XAI_SAVE_METRICS and LLM_METRICS_LOG:
        pd.DataFrame(LLM_METRICS_LOG).to_csv(XAI_METRICS_CSV, index=False)
    print(f"[OK] Wrote results to {XAI_OUTPUT_XLS} ({len(out_df)} rows).")
    if XAI_SAVE_METRICS and LLM_METRICS_LOG:
        print(f"[OK] LLM metrics logged to {XAI_METRICS_CSV} ({len(LLM_METRICS_LOG)} events).")
    return out_df

# =========================
# 9) MAIN
# =========================
def main():
    try:
        df = pd.read_excel(XAI_DATA_PATH, engine="openpyxl")
    except Exception as e:
        raise RuntimeError(f"Failed to read input Excel at {XAI_DATA_PATH}: {e}")
    run_pipeline_from_df(df)

if __name__ == "__main__":
    main()
